{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_412_Vector_Representations_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "U4S1-TEST",
      "language": "python",
      "name": "u4s1-test"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "nteract": {
      "version": "0.14.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s01DbAle4ELu"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Vector Representations\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiR4l6yoVtm6",
        "outputId": "bf395977-4064-4536-fc8b-2b5cf834b728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-28 18:57:49--  https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     137  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-28 18:57:49 (2.75 MB/s) - ‘requirements.txt’ saved [137/137]\n",
            "\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 156kB/s \n",
            "\u001b[?25hCollecting pyLDAvis==2.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 45.0MB/s \n",
            "\u001b[?25hCollecting spacy==2.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 35.3MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/7f/366dcba1ba076a88a50bea732dbc033c0c5bbf7876010e6edc67948579d5/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 43.6MB/s \n",
            "\u001b[?25hCollecting seaborn==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 48.8MB/s \n",
            "\u001b[?25hCollecting squarify==0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/2b/2e77c35326efec19819cd1d729540d4d235e6c2a3f37658288a363a67da5/squarify-0.4.3-py3-none-any.whl\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.1.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (4.6.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.35.1)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.1.3)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (50.3.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 9)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (8.5.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (20.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3->-r requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 7)) (19.0.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 7)) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 3)) (3.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.6.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=ee3f6cd234feb00ea7dd7a0d55a6797090cb13d9c80afe8c3ff56c6b465379f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: gensim, funcy, pyLDAvis, thinc, spacy, scikit-learn, seaborn, squarify\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: seaborn 0.11.0\n",
            "    Uninstalling seaborn-0.11.0:\n",
            "      Successfully uninstalled seaborn-0.11.0\n",
            "Successfully installed funcy-1.15 gensim-3.8.1 pyLDAvis-2.1.2 scikit-learn-0.22.2 seaborn-0.9.0 spacy-2.2.3 squarify-0.4.3 thinc-7.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewsjTzr4VxnQ",
        "outputId": "12ff3dc6-bcdf-4bee-98da-97e9e28670c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.3.1)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=24b55f66e363e46d6ecf73a0b0b765b42f7b999e4fbf6d55b3dc0aa5334d8ed5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l4o1virt/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyj-f9FDcVFp"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03aGTXu6Z5fY"
      },
      "source": [
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqtbj__rbuCS",
        "outputId": "1c9beb67-5e45-47f1-c1ee-8f8f071ac47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/289.txt            \n",
            "  inflating: data/262.txt            \n",
            "  inflating: data/276.txt            \n",
            "  inflating: data/060.txt            \n",
            "  inflating: data/074.txt            \n",
            "  inflating: data/048.txt            \n",
            "  inflating: data/114.txt            \n",
            "  inflating: data/100.txt            \n",
            "  inflating: data/128.txt            \n",
            "  inflating: data/316.txt            \n",
            "  inflating: data/302.txt            \n",
            "  inflating: data/303.txt            \n",
            "  inflating: data/317.txt            \n",
            "  inflating: data/129.txt            \n",
            "  inflating: data/101.txt            \n",
            "  inflating: data/115.txt            \n",
            "  inflating: data/049.txt            \n",
            "  inflating: data/075.txt            \n",
            "  inflating: data/061.txt            \n",
            "  inflating: data/277.txt            \n",
            "  inflating: data/263.txt            \n",
            "  inflating: data/288.txt            \n",
            "  inflating: data/275.txt            \n",
            "  inflating: data/261.txt            \n",
            "  inflating: data/249.txt            \n",
            "  inflating: data/088.txt            \n",
            "  inflating: data/077.txt            \n",
            "  inflating: data/063.txt            \n",
            "  inflating: data/103.txt            \n",
            "  inflating: data/117.txt            \n",
            "  inflating: data/301.txt            \n",
            "  inflating: data/315.txt            \n",
            "  inflating: data/329.txt            \n",
            "  inflating: data/328.txt            \n",
            "  inflating: data/314.txt            \n",
            "  inflating: data/300.txt            \n",
            "  inflating: data/116.txt            \n",
            "  inflating: data/102.txt            \n",
            "  inflating: data/062.txt            \n",
            "  inflating: data/076.txt            \n",
            "  inflating: data/089.txt            \n",
            "  inflating: data/248.txt            \n",
            "  inflating: data/260.txt            \n",
            "  inflating: data/274.txt            \n",
            "  inflating: data/258.txt            \n",
            "  inflating: data/270.txt            \n",
            "  inflating: data/264.txt            \n",
            "  inflating: data/099.txt            \n",
            "  inflating: data/072.txt            \n",
            "  inflating: data/066.txt            \n",
            "  inflating: data/106.txt            \n",
            "  inflating: data/112.txt            \n",
            "  inflating: data/338.txt            \n",
            "  inflating: data/304.txt            \n",
            "  inflating: data/310.txt            \n",
            "  inflating: data/311.txt            \n",
            "  inflating: data/305.txt            \n",
            "  inflating: data/339.txt            \n",
            "  inflating: data/113.txt            \n",
            "  inflating: data/107.txt            \n",
            "  inflating: data/067.txt            \n",
            "  inflating: data/073.txt            \n",
            "  inflating: data/098.txt            \n",
            "  inflating: data/265.txt            \n",
            "  inflating: data/271.txt            \n",
            "  inflating: data/259.txt            \n",
            "  inflating: data/298.txt            \n",
            "  inflating: data/267.txt            \n",
            "  inflating: data/273.txt            \n",
            "  inflating: data/059.txt            \n",
            "  inflating: data/065.txt            \n",
            "  inflating: data/071.txt            \n",
            "  inflating: data/139.txt            \n",
            "  inflating: data/111.txt            \n",
            "  inflating: data/105.txt            \n",
            "  inflating: data/313.txt            \n",
            "  inflating: data/307.txt            \n",
            "  inflating: data/306.txt            \n",
            "  inflating: data/312.txt            \n",
            "  inflating: data/104.txt            \n",
            "  inflating: data/110.txt            \n",
            "  inflating: data/138.txt            \n",
            "  inflating: data/070.txt            \n",
            "  inflating: data/064.txt            \n",
            "  inflating: data/058.txt            \n",
            "  inflating: data/272.txt            \n",
            "  inflating: data/266.txt            \n",
            "  inflating: data/job_listings.csv   \n",
            "  inflating: data/299.txt            \n",
            "  inflating: data/201.txt            \n",
            "  inflating: data/215.txt            \n",
            "  inflating: data/229.txt            \n",
            "  inflating: data/003.txt            \n",
            "  inflating: data/017.txt            \n",
            "  inflating: data/188.txt            \n",
            "  inflating: data/177.txt            \n",
            "  inflating: data/163.txt            \n",
            "  inflating: data/375.txt            \n",
            "  inflating: data/361.txt            \n",
            "  inflating: data/349.txt            \n",
            "  inflating: data/348.txt            \n",
            "  inflating: data/360.txt            \n",
            "  inflating: data/374.txt            \n",
            "  inflating: data/162.txt            \n",
            "  inflating: data/176.txt            \n",
            "  inflating: data/189.txt            \n",
            "  inflating: data/016.txt            \n",
            "  inflating: data/002.txt            \n",
            "  inflating: data/228.txt            \n",
            "  inflating: data/214.txt            \n",
            "  inflating: data/200.txt            \n",
            "  inflating: data/216.txt            \n",
            "  inflating: data/202.txt            \n",
            "  inflating: data/014.txt            \n",
            "  inflating: data/028.txt            \n",
            "  inflating: data/160.txt            \n",
            "  inflating: data/174.txt            \n",
            "  inflating: data/148.txt            \n",
            "  inflating: data/389.txt            \n",
            "  inflating: data/362.txt            \n",
            "  inflating: data/376.txt            \n",
            "  inflating: data/377.txt            \n",
            "  inflating: data/363.txt            \n",
            "  inflating: data/388.txt            \n",
            "  inflating: data/149.txt            \n",
            "  inflating: data/175.txt            \n",
            "  inflating: data/161.txt            \n",
            "  inflating: data/029.txt            \n",
            "  inflating: data/001.txt            \n",
            "  inflating: data/015.txt            \n",
            "  inflating: data/203.txt            \n",
            "  inflating: data/217.txt            \n",
            "  inflating: data/213.txt            \n",
            "  inflating: data/207.txt            \n",
            "  inflating: data/039.txt            \n",
            "  inflating: data/011.txt            \n",
            "  inflating: data/005.txt            \n",
            "  inflating: data/159.txt            \n",
            "  inflating: data/165.txt            \n",
            "  inflating: data/171.txt            \n",
            "  inflating: data/398.txt            \n",
            "  inflating: data/401.txt            \n",
            "  inflating: data/367.txt            \n",
            "  inflating: data/373.txt            \n",
            "  inflating: data/372.txt            \n",
            "  inflating: data/400.txt            \n",
            "  inflating: data/366.txt            \n",
            "  inflating: data/399.txt            \n",
            "  inflating: data/170.txt            \n",
            "  inflating: data/164.txt            \n",
            "  inflating: data/158.txt            \n",
            "  inflating: data/004.txt            \n",
            "  inflating: data/010.txt            \n",
            "  inflating: data/038.txt            \n",
            "  inflating: data/206.txt            \n",
            "  inflating: data/212.txt            \n",
            "  inflating: data/238.txt            \n",
            "  inflating: data/204.txt            \n",
            "  inflating: data/210.txt            \n",
            "  inflating: data/006.txt            \n",
            "  inflating: data/012.txt            \n",
            "  inflating: data/199.txt            \n",
            "  inflating: data/172.txt            \n",
            "  inflating: data/166.txt            \n",
            "  inflating: data/358.txt            \n",
            "  inflating: data/370.txt            \n",
            "  inflating: data/364.txt            \n",
            "  inflating: data/365.txt            \n",
            "  inflating: data/371.txt            \n",
            "  inflating: data/359.txt            \n",
            "  inflating: data/167.txt            \n",
            "  inflating: data/173.txt            \n",
            "  inflating: data/198.txt            \n",
            "  inflating: data/013.txt            \n",
            "  inflating: data/007.txt            \n",
            "  inflating: data/211.txt            \n",
            "  inflating: data/205.txt            \n",
            "  inflating: data/239.txt            \n",
            "  inflating: data/220.txt            \n",
            "  inflating: data/234.txt            \n",
            "  inflating: data/208.txt            \n",
            "  inflating: data/022.txt            \n",
            "  inflating: data/036.txt            \n",
            "  inflating: data/195.txt            \n",
            "  inflating: data/181.txt            \n",
            "  inflating: data/156.txt            \n",
            "  inflating: data/142.txt            \n",
            "  inflating: data/397.txt            \n",
            "  inflating: data/383.txt            \n",
            "  inflating: data/354.txt            \n",
            "  inflating: data/340.txt            \n",
            "  inflating: data/368.txt            \n",
            "  inflating: data/369.txt            \n",
            "  inflating: data/341.txt            \n",
            "  inflating: data/355.txt            \n",
            "  inflating: data/382.txt            \n",
            "  inflating: data/396.txt            \n",
            "  inflating: data/143.txt            \n",
            "  inflating: data/157.txt            \n",
            "  inflating: data/180.txt            \n",
            "  inflating: data/194.txt            \n",
            "  inflating: data/037.txt            \n",
            "  inflating: data/023.txt            \n",
            "  inflating: data/209.txt            \n",
            "  inflating: data/235.txt            \n",
            "  inflating: data/221.txt            \n",
            "  inflating: data/237.txt            \n",
            "  inflating: data/223.txt            \n",
            "  inflating: data/035.txt            \n",
            "  inflating: data/021.txt            \n",
            "  inflating: data/009.txt            \n",
            "  inflating: data/182.txt            \n",
            "  inflating: data/196.txt            \n",
            "  inflating: data/141.txt            \n",
            "  inflating: data/155.txt            \n",
            "  inflating: data/169.txt            \n",
            "  inflating: data/380.txt            \n",
            "  inflating: data/394.txt            \n",
            "  inflating: data/343.txt            \n",
            "  inflating: data/357.txt            \n",
            "  inflating: data/356.txt            \n",
            "  inflating: data/342.txt            \n",
            "  inflating: data/395.txt            \n",
            "  inflating: data/381.txt            \n",
            "  inflating: data/168.txt            \n",
            "  inflating: data/154.txt            \n",
            "  inflating: data/140.txt            \n",
            "  inflating: data/197.txt            \n",
            "  inflating: data/183.txt            \n",
            "  inflating: data/008.txt            \n",
            "  inflating: data/020.txt            \n",
            "  inflating: data/034.txt            \n",
            "  inflating: data/222.txt            \n",
            "  inflating: data/236.txt            \n",
            "  inflating: data/232.txt            \n",
            "  inflating: data/226.txt            \n",
            "  inflating: data/018.txt            \n",
            "  inflating: data/030.txt            \n",
            "  inflating: data/024.txt            \n",
            "  inflating: data/187.txt            \n",
            "  inflating: data/193.txt            \n",
            "  inflating: data/178.txt            \n",
            "  inflating: data/144.txt            \n",
            "  inflating: data/150.txt            \n",
            "  inflating: data/385.txt            \n",
            "  inflating: data/391.txt            \n",
            "  inflating: data/346.txt            \n",
            "  inflating: data/352.txt            \n",
            "  inflating: data/353.txt            \n",
            "  inflating: data/347.txt            \n",
            "  inflating: data/390.txt            \n",
            "  inflating: data/384.txt            \n",
            "  inflating: data/151.txt            \n",
            "  inflating: data/145.txt            \n",
            "  inflating: data/179.txt            \n",
            "  inflating: data/192.txt            \n",
            "  inflating: data/186.txt            \n",
            "  inflating: data/025.txt            \n",
            "  inflating: data/031.txt            \n",
            "  inflating: data/019.txt            \n",
            "  inflating: data/227.txt            \n",
            "  inflating: data/233.txt            \n",
            "  inflating: data/219.txt            \n",
            "  inflating: data/225.txt            \n",
            "  inflating: data/231.txt            \n",
            "  inflating: data/027.txt            \n",
            "  inflating: data/033.txt            \n",
            "  inflating: data/190.txt            \n",
            "  inflating: data/184.txt            \n",
            "  inflating: data/153.txt            \n",
            "  inflating: data/147.txt            \n",
            "  inflating: data/392.txt            \n",
            "  inflating: data/386.txt            \n",
            "  inflating: data/379.txt            \n",
            "  inflating: data/351.txt            \n",
            "  inflating: data/345.txt            \n",
            "  inflating: data/344.txt            \n",
            "  inflating: data/350.txt            \n",
            "  inflating: data/378.txt            \n",
            "  inflating: data/387.txt            \n",
            "  inflating: data/393.txt            \n",
            "  inflating: data/146.txt            \n",
            "  inflating: data/152.txt            \n",
            "  inflating: data/185.txt            \n",
            "  inflating: data/191.txt            \n",
            "  inflating: data/032.txt            \n",
            "  inflating: data/026.txt            \n",
            "  inflating: data/230.txt            \n",
            "  inflating: data/224.txt            \n",
            "  inflating: data/218.txt            \n",
            "  inflating: data/280.txt            \n",
            "  inflating: data/294.txt            \n",
            "  inflating: data/243.txt            \n",
            "  inflating: data/257.txt            \n",
            "  inflating: data/082.txt            \n",
            "  inflating: data/096.txt            \n",
            "  inflating: data/041.txt            \n",
            "  inflating: data/055.txt            \n",
            "  inflating: data/069.txt            \n",
            "  inflating: data/135.txt            \n",
            "  inflating: data/121.txt            \n",
            "  inflating: data/109.txt            \n",
            "  inflating: data/337.txt            \n",
            "  inflating: data/323.txt            \n",
            "  inflating: data/322.txt            \n",
            "  inflating: data/336.txt            \n",
            "  inflating: data/108.txt            \n",
            "  inflating: data/120.txt            \n",
            "  inflating: data/134.txt            \n",
            "  inflating: data/068.txt            \n",
            "  inflating: data/054.txt            \n",
            "  inflating: data/040.txt            \n",
            "  inflating: data/097.txt            \n",
            "  inflating: data/083.txt            \n",
            "  inflating: data/256.txt            \n",
            "  inflating: data/242.txt            \n",
            "  inflating: data/295.txt            \n",
            "  inflating: data/281.txt            \n",
            "  inflating: data/297.txt            \n",
            "  inflating: data/283.txt            \n",
            "  inflating: data/254.txt            \n",
            "  inflating: data/240.txt            \n",
            "  inflating: data/268.txt            \n",
            "  inflating: data/095.txt            \n",
            "  inflating: data/081.txt            \n",
            "  inflating: data/056.txt            \n",
            "  inflating: data/042.txt            \n",
            "  inflating: data/122.txt            \n",
            "  inflating: data/136.txt            \n",
            "  inflating: data/320.txt            \n",
            "  inflating: data/334.txt            \n",
            "  inflating: data/308.txt            \n",
            "  inflating: data/309.txt            \n",
            "  inflating: data/335.txt            \n",
            "  inflating: data/321.txt            \n",
            "  inflating: data/137.txt            \n",
            "  inflating: data/123.txt            \n",
            "  inflating: data/043.txt            \n",
            "  inflating: data/057.txt            \n",
            "  inflating: data/080.txt            \n",
            "  inflating: data/094.txt            \n",
            "  inflating: data/269.txt            \n",
            "  inflating: data/241.txt            \n",
            "  inflating: data/255.txt            \n",
            "  inflating: data/282.txt            \n",
            "  inflating: data/296.txt            \n",
            "  inflating: data/292.txt            \n",
            "  inflating: data/286.txt            \n",
            "  inflating: data/279.txt            \n",
            "  inflating: data/251.txt            \n",
            "  inflating: data/245.txt            \n",
            "  inflating: data/090.txt            \n",
            "  inflating: data/084.txt            \n",
            "  inflating: data/053.txt            \n",
            "  inflating: data/047.txt            \n",
            "  inflating: data/127.txt            \n",
            "  inflating: data/133.txt            \n",
            "  inflating: data/319.txt            \n",
            "  inflating: data/325.txt            \n",
            "  inflating: data/331.txt            \n",
            "  inflating: data/330.txt            \n",
            "  inflating: data/324.txt            \n",
            "  inflating: data/318.txt            \n",
            "  inflating: data/132.txt            \n",
            "  inflating: data/126.txt            \n",
            "  inflating: data/046.txt            \n",
            "  inflating: data/052.txt            \n",
            "  inflating: data/085.txt            \n",
            "  inflating: data/091.txt            \n",
            "  inflating: data/244.txt            \n",
            "  inflating: data/250.txt            \n",
            "  inflating: data/278.txt            \n",
            "  inflating: data/287.txt            \n",
            "  inflating: data/293.txt            \n",
            "  inflating: data/285.txt            \n",
            "  inflating: data/291.txt            \n",
            "  inflating: data/246.txt            \n",
            "  inflating: data/252.txt            \n",
            "  inflating: data/087.txt            \n",
            "  inflating: data/093.txt            \n",
            "  inflating: data/078.txt            \n",
            "  inflating: data/044.txt            \n",
            "  inflating: data/050.txt            \n",
            "  inflating: data/118.txt            \n",
            "  inflating: data/130.txt            \n",
            "  inflating: data/124.txt            \n",
            "  inflating: data/332.txt            \n",
            "  inflating: data/326.txt            \n",
            "  inflating: data/327.txt            \n",
            "  inflating: data/333.txt            \n",
            "  inflating: data/125.txt            \n",
            "  inflating: data/131.txt            \n",
            "  inflating: data/119.txt            \n",
            "  inflating: data/051.txt            \n",
            "  inflating: data/045.txt            \n",
            "  inflating: data/079.txt            \n",
            "  inflating: data/092.txt            \n",
            "  inflating: data/086.txt            \n",
            "  inflating: data/253.txt            \n",
            "  inflating: data/247.txt            \n",
            "  inflating: data/290.txt            \n",
            "  inflating: data/284.txt            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy2-Gi00e75i",
        "outputId": "83287499-8517-4918-ec23-35d19d7c1793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  data.zip\trequirements.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7bcmqfGXrFG"
      },
      "source": [
        "## 1) *Clean:* Job Listings from indeed.com that contain the title \"Data Scientist\" \n",
        "\n",
        "You have `job_listings.csv` in the data folder for this module. The text data in the description column is still messy - full of html tags. Use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to clean up this column. You will need to read through the documentation to accomplish this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcYlc1URXhlC",
        "outputId": "ec2965ef-603f-4c37-d100-e792042ba49e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "##### Your Code Here #####\n",
        "df = pd.read_csv('data/job_listings.csv').drop(columns=\"Unnamed: 0\")\n",
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b\"&lt;div&gt;&lt;div&gt;Job Requirements:&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;...</td>\n",
              "      <td>Data scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'&lt;div&gt;Job Description&lt;br/&gt;\\n&lt;br/&gt;\\n&lt;p&gt;As a Da...</td>\n",
              "      <td>Data Scientist I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'&lt;div&gt;&lt;p&gt;As a Data Scientist you will be work...</td>\n",
              "      <td>Data Scientist - Entry Level</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'&lt;ul&gt;&lt;li&gt;Location: USA \\xe2\\x80\\x93 multiple ...</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>b\"&lt;b&gt;About Us:&lt;/b&gt;&lt;br/&gt;\\nWant to be part of a ...</td>\n",
              "      <td>Senior Data Science Engineer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>2019 PhD Data Scientist Internship - Forecasti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>Data Scientist - Insurance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>b\"&lt;p&gt;&lt;/p&gt;&lt;div&gt;&lt;p&gt;SENIOR DATA SCIENTIST&lt;/p&gt;&lt;p&gt;\\...</td>\n",
              "      <td>Senior Data Scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>b'&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Cerner Int...</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           description                                              title\n",
              "0    b\"<div><div>Job Requirements:</div><ul><li><p>...                                    Data scientist \n",
              "1    b'<div>Job Description<br/>\\n<br/>\\n<p>As a Da...                                   Data Scientist I\n",
              "2    b'<div><p>As a Data Scientist you will be work...                       Data Scientist - Entry Level\n",
              "3    b'<div class=\"jobsearch-JobMetadataHeader icl-...                                     Data Scientist\n",
              "4    b'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ...                                     Data Scientist\n",
              "..                                                 ...                                                ...\n",
              "421  b\"<b>About Us:</b><br/>\\nWant to be part of a ...                       Senior Data Science Engineer\n",
              "422  b'<div class=\"jobsearch-JobMetadataHeader icl-...  2019 PhD Data Scientist Internship - Forecasti...\n",
              "423  b'<div class=\"jobsearch-JobMetadataHeader icl-...                         Data Scientist - Insurance\n",
              "424  b\"<p></p><div><p>SENIOR DATA SCIENTIST</p><p>\\...                              Senior Data Scientist\n",
              "425  b'<div></div><div><div><div><div><p>Cerner Int...                                     Data Scientist\n",
              "\n",
              "[426 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLaDuZoMi_Jk"
      },
      "source": [
        "def get_text(text):\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "  return soup.get_text()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlweFV5EirGf",
        "outputId": "e4c44430-1d75-410d-e70f-e64f52d8710f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df['text'] = df['description'].apply(lambda x:BeautifulSoup(x, 'html.parser').get_text()[2:-1].replace('\\\\n', ' ') )\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b\"&lt;div&gt;&lt;div&gt;Job Requirements:&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;...</td>\n",
              "      <td>Data scientist</td>\n",
              "      <td>Job Requirements: Conceptual understanding in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'&lt;div&gt;Job Description&lt;br/&gt;\\n&lt;br/&gt;\\n&lt;p&gt;As a Da...</td>\n",
              "      <td>Data Scientist I</td>\n",
              "      <td>Job Description  As a Data Scientist 1, you wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'&lt;div&gt;&lt;p&gt;As a Data Scientist you will be work...</td>\n",
              "      <td>Data Scientist - Entry Level</td>\n",
              "      <td>As a Data Scientist you will be working on con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>$4,969 - $6,756 a monthContractUnder the gener...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'&lt;ul&gt;&lt;li&gt;Location: USA \\xe2\\x80\\x93 multiple ...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Location: USA \\xe2\\x80\\x93 multiple locations ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>b\"&lt;b&gt;About Us:&lt;/b&gt;&lt;br/&gt;\\nWant to be part of a ...</td>\n",
              "      <td>Senior Data Science Engineer</td>\n",
              "      <td>About Us: Want to be part of a fantastic and f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>2019 PhD Data Scientist Internship - Forecasti...</td>\n",
              "      <td>InternshipAt Uber, we ignite opportunity by se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>Data Scientist - Insurance</td>\n",
              "      <td>$200,000 - $350,000 a yearA million people a y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>b\"&lt;p&gt;&lt;/p&gt;&lt;div&gt;&lt;p&gt;SENIOR DATA SCIENTIST&lt;/p&gt;&lt;p&gt;\\...</td>\n",
              "      <td>Senior Data Scientist</td>\n",
              "      <td>SENIOR DATA SCIENTIST JOB DESCRIPTION  ABOUT U...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>b'&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Cerner Int...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Cerner Intelligence is a new, innovative organ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           description  ...                                               text\n",
              "0    b\"<div><div>Job Requirements:</div><ul><li><p>...  ...  Job Requirements: Conceptual understanding in ...\n",
              "1    b'<div>Job Description<br/>\\n<br/>\\n<p>As a Da...  ...  Job Description  As a Data Scientist 1, you wi...\n",
              "2    b'<div><p>As a Data Scientist you will be work...  ...  As a Data Scientist you will be working on con...\n",
              "3    b'<div class=\"jobsearch-JobMetadataHeader icl-...  ...  $4,969 - $6,756 a monthContractUnder the gener...\n",
              "4    b'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ...  ...  Location: USA \\xe2\\x80\\x93 multiple locations ...\n",
              "..                                                 ...  ...                                                ...\n",
              "421  b\"<b>About Us:</b><br/>\\nWant to be part of a ...  ...  About Us: Want to be part of a fantastic and f...\n",
              "422  b'<div class=\"jobsearch-JobMetadataHeader icl-...  ...  InternshipAt Uber, we ignite opportunity by se...\n",
              "423  b'<div class=\"jobsearch-JobMetadataHeader icl-...  ...  $200,000 - $350,000 a yearA million people a y...\n",
              "424  b\"<p></p><div><p>SENIOR DATA SCIENTIST</p><p>\\...  ...  SENIOR DATA SCIENTIST JOB DESCRIPTION  ABOUT U...\n",
              "425  b'<div></div><div><div><div><div><p>Cerner Int...  ...  Cerner Intelligence is a new, innovative organ...\n",
              "\n",
              "[426 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I-hrUJQrrUw"
      },
      "source": [
        "import string"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43VF3v4AlQOU",
        "outputId": "d7fa1e87-6908-4903-e683-86f0ed771ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "for row in df['text'].head(10):\n",
        "  print(row)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job Requirements: Conceptual understanding in Machine Learning models like Nai\\xc2\\xa8ve Bayes, K-Means, SVM, Apriori, Linear/ Logistic Regression, Neural, Random Forests, Decision Trees, K-NN along with hands-on experience in at least 2 of them Intermediate to expert level coding skills in Python/R. (Ability to write functions, clean and efficient data manipulation are mandatory for this role) Exposure to packages like NumPy, SciPy, Pandas, Matplotlib etc in Python or GGPlot2, dplyr, tidyR in R Ability to communicate Model findings to both Technical and Non-Technical stake holders Hands on experience in SQL/Hive or similar programming language Must show past work via GitHub, Kaggle or any other published article Master's degree in Statistics/Mathematics/Computer Science or any other quant specific field. Apply Now\n",
            "Job Description  As a Data Scientist 1, you will help us build machine learning models, data pipelines, and micro-services to help our clients navigate their healthcare journey. You will do so by empowering and improving the next generation of Accolade Applications and user experiences. A day in the life\\xe2\\x80\\xa6 Work with a small agile team to design and develop mobile applications in an iterative fashion. Work with a tight-knit group of development team members in Seattle. Contribute to best practices and help guide the future of our applications. Operates effectively as a collaborative member of the development team. Operates effectively as an individual for quick turnaround of enhancements and fixes. Responsible for meeting expectations and deliverables on time with high quality. Drive and implement new features within our mobile applications. Perform thorough manual testing and writing test cases that cover all areas. Identify new development tools/approaches that will increase code quality, efficiency, and best practices. Develop and champion the the development processes, coding style guidelines, and architectural designs necessary to innovate and maintain great product quality. Effectively turns design documents and graphics into performant, usable UI. Demonstrates creative, technical, and analytical skills. Demonstrates ability to communicate effectively in both technical and business environments  Qualifications  What we are looking for\\xe2\\x80\\xa6 Master\\xe2\\x80\\x99s Degree in Computer Science, Math, or related field. Computer Science fundamentals, as illustrated through algorithm design, problem solving, and complexity analysis. Must have 1+ year real-world experience developing and deploying micro-services or data pipelines Must have a fundamental understanding of key machine learning concepts, such as accuracy measures, cross-validation, and open source machine learning libraries Fluent in Python and SQL Proficient with writing unit/functional tests and familiar with automation frameworks Experience with cloud infrastructure, such as AWS or Azure, is a plus. Experience with distributed data pipelines, such as a Spark, is a plus. Strong written and oral communication skills. Desire and willingness to work in an Agile, collaborative, innovative, flexible, and team-oriented environment Hands-on, detail-oriented, methodical & inquisitive A motivated self-starter with a solid level of experience that quickly grasps complex challenges A skillful communicator with experience working with technical management teams  A service oriented person who thinks \"Customer First\" Fast fail entrepreneurial spirit Thrives in a fast-paced environment where continuous improvement is the norm and the bar for quality is extremely high Excited by the challenges of working in a product team undergoing rapid, international growth Additional Information  What is important to us Creating an enduring company that is hyper-focused on our culture and making a meaningful impact in the lives of our employees, members and customers. The secret to our success is: We find joy and purpose in serving others Making a difference in our members\\xe2\\x80\\x99 and customers\\xe2\\x80\\x99 lives is what we do. Even when it\\xe2\\x80\\x99s hard, we do the right thing for the right reasons. We are strong individually and together, we\\xe2\\x80\\x99re powerful Trusting in our colleagues and embracing their different backgrounds and experiences enable us to solve tough problems in creative ways, having fun along the way. We roll up our sleeves and get stuff done Results motivate us. And we aren\\'t afraid of the hard work or tough decisions needed to get us there. We\\xe2\\x80\\x99re boldly and relentlessly reinventing healthcare We\\'re curious and act big - not afraid to knock down barriers or take calculated risks to change the world, one person at a time. All your information will be kept confidential according to EEO guidelines.\n",
            "As a Data Scientist you will be working on consulting side of our business. You will be responsible for analyzing large, complex datasets and identify meaningful patterns that lead to actionable recommendations. You will be performing thorough testing and validation of models, and support various aspects of the business with data analytics. Ability to do statistical modeling, build predictive models and leverage machine learning algorithms. This position will combine the typical Data Scientist math and analytical skills, with research, advanced business, communication, and presentation skills. Primary job location is in Sacramento, but work-from-home option is available.  Qualifications Bachelors, MS or PhD in a relevant field (Computer Science, Engineering, Statistics, Physics, Applied Math) Experience in R and/or Python is preferred\n",
            "$4,969 - $6,756 a monthContractUnder the general supervision of Professors Dana Mukamel and Kai Zheng, the incumbent will join the CalMHSA Mental Health Tech Suite Innovation (INN) Evaluation Team. This large, statewide multi-year study will evaluate the effectiveness of two new and innovative applications offered to people with mental health conditions, which include opportunities for online chatting between users and online listeners Responsibilities of the incumbent will include managing and analyzing text data created by users of the two mental health applications as part of the research and evaluation objectives of the team. The incumbent will collaborate with faculty and other team researchers, and will be expected to create under supervision and direction variables describing the usage of the apps, the interactions between users, and the effectiveness of the apps. The incumbent will also be expected to interact with the vendors of the apps around data issues.  The University of California, Irvine is an Equal Opportunity/Affirmative Action Employer advancing inclusive excellence. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, protected veteran status, or other protected categories covered by the UC nondiscrimination policy.  Salary: Monthly $4,968.58 - $6,755.83 Total Hours: 8-5, M-F Contract Position. Final candidate subject to background check. As a federal contractor, UC Irvine is required to use E-Verify to confirm the work status of individuals assigned to perform substantial work under certain federal contracts/subcontracts.  Please attach your resume.\n",
            "Location: USA \\xe2\\x80\\x93 multiple locations 2+ years of Analytics experience Understand business requirements and technical requirements Can handle data extraction, preparation and transformation Create and implement data models\n",
            "Create various Business Intelligence Analytical reports, visualization and dashboards with BI tools like Tableau, Power BI or similar; Utilize experience in scientific data, logic programming and calculated columns, and decision making; Develop and maintain dashboards for KPIs, purchase trends with time series and customer flows with Tableau and Teradata; Develop recommendation models utilizing machine learning and predictive analysis with supervised and unsupervised algorithms like random forest, support vector machine and k-means clustering; Utilize experience with SQL with strong concepts of database, data warehouse and metadata; Work closely with frontend web developer and UX designer on improving online shopping experience and deploying business strategies like promotion or similar; Analyze customer behaviors and purchase trends to create customized recommended items; Conduct and apply A/B test to monitor and test new or modified features for online shopping experiences across desktop and mobile platforms; Collect real time data from A/B test and generate feedback reports for presentations and communications to other teams, both technical and non-technical; Utilize clustering models to categorize customers and generate optimized business strategies for different categories; Create novel computational approaches and analytical tools as required by market and business goals; Design databases and develop algorithms for processing and analyzing purchase, device information and customer information; Analyze geographic trends for online shopper sessions with business strategy implications and present on dashboard; Consult with clients to analyze business problem, setup market goals, recommend technology-based solutions, or determine computational strategies; Work with large data access, admin & configuration and application data services; Work with business stakeholders to identify requirements and outcomes, and to frame meaningful business scenarios that impact critical business functions; Design experiments, test hypothesis, build models to conduct data analysis and design algorithms and utilize appropriate designs to conduct analytics and discover patterns; Key Skills: Python, R, SQL, Tableau, Excel\n",
            "As Spotify Premium swells to over 96M subscribers around the globe, we are looking for new ways to continue to grow our subscription business. You would be joining Spotify on the Premium Analytics team, a core business strategy and insights team, as an Associate Data Scientist. In this unique position, your work would be essential in shaping how Spotify is able to grow through data-driven recommendations, new product offerings and innovative marketing efforts. You will see first hand how your work translates into new strategies, products, and consumer experiences as we enter a new phase in Spotify Premium\\xe2\\x80\\x99s life.  You will work with a global team of world-class analysts, data scientists, business managers, marketers, and engineers. We are all passionate about what we do and move forward with high impact projects at a high pace. Learning and improving is part of our daily routine, and you will be free to develop your own skills and ways of working. At your fingertips you\\xe2\\x80\\x99ll have access to petabytes of data, and will get the opportunity to be creative with how you drive insights and strategies from that. Above all, your work will impact the way the world experiences music.  What You\\xe2\\x80\\x99ll Do  Develop data-driven strategies to drive the growth of Spotify subscribers Create and communicate actionable recommendations that improve our product conversion and migration metrics. Work with everything from advanced algorithmic data analysis and AB-test setup to business analysis and modeling Work closely with business stakeholders to understand the change they are driving and help them discover new opportunities for growth. Who Are You  You are an open-minded, creative person with an interest in analyses and data science You have some professional experience working with data analysis (~1 year) You are comfortable with the whole analytical process from identifying insight gaps to designing and running initiatives to fill them You have worked hands-on synthesizing insights from data using tools such as Python, R, SQL, SAS, SPSS, Minitab and/or Hadoop You are a communicative person that values building strong relationships with colleagues and partners, you are experienced in presenting insights and recommendations to partners or clients\n",
            "Everytown for Gun Safety, the nation's largest gun violence prevention advocacy group, is seeking a Data Scientist. The Research Department studies how gun violence occurs in America and identifies the best policies and interventions to prevent and address it. In this role, the Data Scientist will need to understand and communicate about the various types of gun violence, who it impacts, and how it impacts different communities and populations differently. You must be committed to addressing gun violence in diverse communities around the country.  The Data Scientist will be the policy data lead for the organization, responsible for advanced data management, analysis, and visualization. The Data Scientist reports to the Principal Researcher.  We'll trust you to take on the following responsibilities:  Serve as the lead data analyst on the team, responsible for the accuracy and appropriate use of all calculations and findings Conduct original analysis and identify new avenues for statistical analysis on gun violence prevention, policies, and interventions to support Everytown for Gun Safety advocacy and research products and campaigns Analyze large datasets using various software packages (R, STATA, SPSS, SQL, Python, and/or others) Oversee staff to obtain, organize, and update datasets of frequent reference (e.g., CDC fatal and nonfatal injury data; FBI Supplementary Homicide Report data; FBI background check data; and other federal-, state-, and city-level data related to gun violence prevention) Optimize and ensure integrity of all databases managed across the organization Lead efforts to identify new datasets and opportunities for original analysis, as well as their application across Departments Direct and implement data visualization efforts and review all products from a data perspective Serve as the subject matter expert on economic analysis of gun violence Serve as a mentor to the team and identify training and development opportunities Contribute to the publication of analytical reports, compelling advocacy materials, and other research-based products to support Everytown's legislative and outreach/advocacy goals Clearly translate the implications of research findings (from Everytown original research and the work of scholars in the field) so that practitioners and policymakers can implement new laws effectively and enforce existing policies  The ideal candidate will have:  Advanced degree in a quantitative field or equivalent professional experience 5+ years applied experience in research, policy, and/or economics; economics experience preferred Fluency with multiple research methods, both quantitative and qualitative, such as multivariate statistics, cluster analysis, survey research, regression analysis, classification modelling, hypothesis testing, and/or composite index creation Experience working with large datasets using at least one software package (R, STATA, SPSS, SQL, Python, and/or others) Significant data management and data visualization experience Familiarity with government-generated data and information sources (city, state, and federal), e.g. U.S. Census data, preferred Demonstrated ability to effectively communicate research findings to a broad range of audiences The ability to work in a dynamic and fast paced environment with an open floor plan  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.  Everytown for Gun Safety provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws.  Candidates who identify as members of historically underrepresented groups are highly encouraged to apply. A diverse workforce and open culture are at the heart of our organization and vital to our success.\n",
            "MS in a quantitative discipline such as Statistics, Mathematics, Physics, Engineering, Computer Science or Economics5+ years work experienceProficiency in at least one statistical software package such as Python, R or MatlabExpertise using SQL for acquiring and transforming dataOutstanding quantitative modeling and statistical analysis skillsExcellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and business audiences  Where will Amazon's growth come from in the next year? What about over the next five? Which product lines are poised to quintuple in size? Are we investing enough in our infrastructure, or too much? How do our customers react to changes in prices, product selection, or delivery times? These are among the most important questions at Amazon today. The Topline Forecast team in the Supply Chain Optimization Technologies (SCOT) organization is dedicated to answering these questions using statistical methods. We develop cutting edge data pipelines, build accurate predictive models, and deploy automated software solutions to provide forecasting insights to business leaders at the most senior levels throughout the company. We are looking for a talented, driven, and analytical researcher to help us answer these (and many more) questions. This Data Scientist role will design quantitative systems and forecasting models that generate multi-billion dollar predictions of the highest level of visibility and importance for Amazon's financial and operational planning. A successful candidate will be a problem solver who enjoys diving into data, is excited by difficult modeling challenges, and possesses strong communication skills to effectively interface between technical and business teams. As a Data Scientist on the Topline team, you will collaborate directly with economists and statisticians to produce modeling solutions, you will partner with software developers and data engineers to build end-to-end data pipelines and production code, and you will have exposure to senior leadership as we communicate results and provide scientific guidance to the business. Key Responsibilities Implement statistical and machine learning methods to solve specific business problemsImprove upon existing methodologies by developing new data sources, testing model enhancements, and fine-tuning model parametersDirectly contribute to the design and development of automated forecasting systemsBuild customer-facing reporting tools to provide insights and metrics which track forecast performance and explain varianceCollaborate with researchers, software developers, and business leaders to define product requirements, provide analytical support, and communicate feedback Amazon is an Equal Opportunity Employer \\xe2\\x80\\x93 Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.  Experience building complex data visualizationsExperience working in command-line Linux environmentsExperience with object-oriented programming languagesExperience with causal inference, applied time series modeling or machine learning forecasting applicationsStrong project management skills\n",
            "Slack is hiring experienced data scientists to join our Lifecycle team. The Lifecycle team\\xe2\\x80\\x99s mission is to build product experiences that help companies of all sizes adopt and scale Slack within their organization. These product experiences include team and user onboarding, payments and billing systems, and the marketing and packaging of paid plans.  You will use data to help the team discover opportunities and define appropriate solutions to user problems by performing research into user behavior, defining and applying experimentation standards, and understanding the drivers of business performance.  Slack has a positive, diverse, and supportive culture\\xe2\\x80\\x94we look for people who are curious, inventive, and work to be a little better every single day. In our work together we aim to be smart, humble, hardworking and, above all, collaborative. If this sounds like a good fit for you, why not say hello?  What you will be doing  Use data to influence the direction of team roadmaps and inform business decisions Deepen our understanding of our product, our users, and our business through data and the use of the scientific method Work with partner teams to define goals and identify metrics that describe our product through data Be an ambassador for data by improving the availability, understanding, and sophistication with data at Slack What you should have  6+ years of professional industry experience doing quantitative analysis A consistent record of using analysis to impact key business or product decisions The ability to clearly and effectively communicate the results of complex analyses Understanding of standard data science methodologies Deep understanding of statistics Ability to write clean code, especially in R or Python   Slack is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Women, minorities, individuals with disabilities and protected veterans are encouraged to apply. Slack will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance.  Slack is a layer of the business technology stack that brings together people, data, and applications \\xe2\\x80\\x93 a single place where people can effectively work together, find important information, and access hundreds of thousands of critical applications and services to do their best work. From global Fortune 100 companies to corner markets, businesses and teams of all kinds use Slack to bring the right people together with all the right information. Slack is headquartered in San Francisco, CA and has ten offices around the world. For more information on how Slack makes teams better connected, visit slack.com. Ensuring a diverse and inclusive workplace where we learn from each other is core to Slack\\xe2\\x80\\x99s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a pleasant and supportive place to work. Come do the best work of your life here at Slack.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4xFZNtX1m2"
      },
      "source": [
        "## 2) Use Spacy to tokenize the listings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-IgSp4uw9gB"
      },
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhUHuMr-X-II"
      },
      "source": [
        "##### Your Code Here #####\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def tokenizer_func(df_column):\n",
        "  tokens = []\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  #token.is_punct == False\n",
        "\n",
        "  extend_stop_words = [' ', '']\n",
        "  STOP_WORDS = nlp.Defaults.stop_words.union(extend_stop_words)\n",
        "\n",
        "  for doc in tokenizer.pipe(df_column, batch_size=500):\n",
        "    doc_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "      word = token.lemma_.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "      #word = ps.stem(word)\n",
        "      if(word not in STOP_WORDS) and (token.pos_ != 'PRON'):\n",
        "        doc_tokens.append(word)\n",
        "\n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvyDqIMwwlml",
        "outputId": "33de2db1-fa73-40d2-c50d-588b9f098e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df['tokens_spacy'] = tokenizer_func(df['text'])\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>tokens_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b\"&lt;div&gt;&lt;div&gt;Job Requirements:&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;...</td>\n",
              "      <td>Data scientist</td>\n",
              "      <td>Job Requirements: Conceptual understanding in ...</td>\n",
              "      <td>[job, requirements, conceptual, understand, ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'&lt;div&gt;Job Description&lt;br/&gt;\\n&lt;br/&gt;\\n&lt;p&gt;As a Da...</td>\n",
              "      <td>Data Scientist I</td>\n",
              "      <td>Job Description  As a Data Scientist 1, you wi...</td>\n",
              "      <td>[job, description, data, scientist, 1, help, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'&lt;div&gt;&lt;p&gt;As a Data Scientist you will be work...</td>\n",
              "      <td>Data Scientist - Entry Level</td>\n",
              "      <td>As a Data Scientist you will be working on con...</td>\n",
              "      <td>[data, scientist, work, consult, business, res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>$4,969 - $6,756 a monthContractUnder the gener...</td>\n",
              "      <td>[4969, 6756, monthcontractunder, general, supe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'&lt;ul&gt;&lt;li&gt;Location: USA \\xe2\\x80\\x93 multiple ...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Location: USA \\xe2\\x80\\x93 multiple locations ...</td>\n",
              "      <td>[location, usa, xe2x80x93, multiple, location,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>b\"&lt;b&gt;About Us:&lt;/b&gt;&lt;br/&gt;\\nWant to be part of a ...</td>\n",
              "      <td>Senior Data Science Engineer</td>\n",
              "      <td>About Us: Want to be part of a fantastic and f...</td>\n",
              "      <td>[want, fantastic, fun, startup, thatxe2x80x99s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>2019 PhD Data Scientist Internship - Forecasti...</td>\n",
              "      <td>InternshipAt Uber, we ignite opportunity by se...</td>\n",
              "      <td>[internshipat, uber, ignite, opportunity, set,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>b'&lt;div class=\"jobsearch-JobMetadataHeader icl-...</td>\n",
              "      <td>Data Scientist - Insurance</td>\n",
              "      <td>$200,000 - $350,000 a yearA million people a y...</td>\n",
              "      <td>[200000, 350000, yeara, million, people, year,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>b\"&lt;p&gt;&lt;/p&gt;&lt;div&gt;&lt;p&gt;SENIOR DATA SCIENTIST&lt;/p&gt;&lt;p&gt;\\...</td>\n",
              "      <td>Senior Data Scientist</td>\n",
              "      <td>SENIOR DATA SCIENTIST JOB DESCRIPTION  ABOUT U...</td>\n",
              "      <td>[senior, data, scientist, job, description, am...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>b'&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Cerner Int...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Cerner Intelligence is a new, innovative organ...</td>\n",
              "      <td>[cerner, intelligence, new, innovative, organi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           description  ...                                       tokens_spacy\n",
              "0    b\"<div><div>Job Requirements:</div><ul><li><p>...  ...  [job, requirements, conceptual, understand, ma...\n",
              "1    b'<div>Job Description<br/>\\n<br/>\\n<p>As a Da...  ...  [job, description, data, scientist, 1, help, b...\n",
              "2    b'<div><p>As a Data Scientist you will be work...  ...  [data, scientist, work, consult, business, res...\n",
              "3    b'<div class=\"jobsearch-JobMetadataHeader icl-...  ...  [4969, 6756, monthcontractunder, general, supe...\n",
              "4    b'<ul><li>Location: USA \\xe2\\x80\\x93 multiple ...  ...  [location, usa, xe2x80x93, multiple, location,...\n",
              "..                                                 ...  ...                                                ...\n",
              "421  b\"<b>About Us:</b><br/>\\nWant to be part of a ...  ...  [want, fantastic, fun, startup, thatxe2x80x99s...\n",
              "422  b'<div class=\"jobsearch-JobMetadataHeader icl-...  ...  [internshipat, uber, ignite, opportunity, set,...\n",
              "423  b'<div class=\"jobsearch-JobMetadataHeader icl-...  ...  [200000, 350000, yeara, million, people, year,...\n",
              "424  b\"<p></p><div><p>SENIOR DATA SCIENTIST</p><p>\\...  ...  [senior, data, scientist, job, description, am...\n",
              "425  b'<div></div><div><div><div><div><p>Cerner Int...  ...  [cerner, intelligence, new, innovative, organi...\n",
              "\n",
              "[426 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgCZNL_YycP"
      },
      "source": [
        "## 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg7A9g4hvo1e"
      },
      "source": [
        "def get_word_counts(text):\n",
        "  extend_stop_words = [' ', '', 'xe2', 'x80', 'data', 'x99s']\n",
        "  STOP_WORDS = nlp.Defaults.stop_words.union(extend_stop_words)\n",
        "  vect = CountVectorizer(stop_words=STOP_WORDS, min_df=3)\n",
        "  vect.fit(text)\n",
        "\n",
        "  dtm = vect.transform(text)\n",
        "  return pd.DataFrame(data=dtm.todense(), columns=vect.get_feature_names())"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqutHh6mx2RY",
        "outputId": "da4df164-d582-4fba-dbf1-7d62e2f2d91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "word_counts = get_word_counts(df['text'])\n",
        "word_counts"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>04</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1079302</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>150</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>200</th>\n",
              "      <th>2012</th>\n",
              "      <th>2013</th>\n",
              "      <th>2015</th>\n",
              "      <th>2017</th>\n",
              "      <th>2018</th>\n",
              "      <th>2019</th>\n",
              "      <th>2020</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>30</th>\n",
              "      <th>300</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>3rd</th>\n",
              "      <th>3x</th>\n",
              "      <th>40</th>\n",
              "      <th>401</th>\n",
              "      <th>401k</th>\n",
              "      <th>43</th>\n",
              "      <th>45</th>\n",
              "      <th>50</th>\n",
              "      <th>500</th>\n",
              "      <th>55</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>...</th>\n",
              "      <th>worldwide</th>\n",
              "      <th>worth</th>\n",
              "      <th>wrangler</th>\n",
              "      <th>wrangling</th>\n",
              "      <th>write</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>www</th>\n",
              "      <th>x83</th>\n",
              "      <th>x84</th>\n",
              "      <th>x93</th>\n",
              "      <th>x94</th>\n",
              "      <th>x94and</th>\n",
              "      <th>x94including</th>\n",
              "      <th>x94to</th>\n",
              "      <th>x94we</th>\n",
              "      <th>x98</th>\n",
              "      <th>x98big</th>\n",
              "      <th>x99</th>\n",
              "      <th>x99d</th>\n",
              "      <th>x99ll</th>\n",
              "      <th>x99re</th>\n",
              "      <th>x99t</th>\n",
              "      <th>x99ve</th>\n",
              "      <th>x9cbig</th>\n",
              "      <th>x9d</th>\n",
              "      <th>xa2</th>\n",
              "      <th>xa6</th>\n",
              "      <th>xae</th>\n",
              "      <th>xbb</th>\n",
              "      <th>xc2</th>\n",
              "      <th>xc3</th>\n",
              "      <th>xef</th>\n",
              "      <th>xgboost</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yearthe</th>\n",
              "      <th>yes</th>\n",
              "      <th>york</th>\n",
              "      <th>yrs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426 rows × 3876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     000  04  10  100  1079302  11  ...  year  years  yearthe  yes  york  yrs\n",
              "0      0   0   0    0        0   0  ...     0      0        0    0     0    0\n",
              "1      0   0   0    0        0   0  ...     1      0        0    0     0    0\n",
              "2      0   0   0    0        0   0  ...     0      0        0    0     0    0\n",
              "3      0   0   0    0        0   0  ...     1      0        0    0     0    0\n",
              "4      0   0   0    0        0   0  ...     0      1        0    0     0    0\n",
              "..   ...  ..  ..  ...      ...  ..  ...   ...    ...      ...  ...   ...  ...\n",
              "421    0   0   0    0        0   0  ...     2      3        0    0     1    0\n",
              "422    0   0   0    0        0   0  ...     0      0        0    0     0    0\n",
              "423    2   0   0    0        0   0  ...     1      0        0    0     0    0\n",
              "424    0   0   0    0        0   0  ...     0      1        0    0     0    0\n",
              "425    1   0   0    0        0   0  ...     0      2        0    1     0    0\n",
              "\n",
              "[426 rows x 3876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcLDiq3_yJAl",
        "outputId": "3766473c-59e4-4017-961b-adce6dedd915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "word_counts.sum().sort_values(ascending=False).head(40)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "experience     1854\n",
              "business       1212\n",
              "work           1161\n",
              "team            988\n",
              "science         958\n",
              "learning        919\n",
              "analytics       736\n",
              "machine         699\n",
              "skills          697\n",
              "analysis        680\n",
              "models          617\n",
              "product         583\n",
              "statistical     576\n",
              "solutions       530\n",
              "new             529\n",
              "ability         513\n",
              "scientist       511\n",
              "insights        464\n",
              "help            458\n",
              "technical       453\n",
              "working         448\n",
              "python          447\n",
              "tools           444\n",
              "years           444\n",
              "engineering     425\n",
              "modeling        423\n",
              "knowledge       419\n",
              "development     411\n",
              "company         404\n",
              "statistics      390\n",
              "techniques      389\n",
              "advanced        387\n",
              "teams           384\n",
              "develop         382\n",
              "research        381\n",
              "strong          373\n",
              "world           372\n",
              "degree          370\n",
              "including       370\n",
              "opportunity     370\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo1iH_UeY7_n"
      },
      "source": [
        "## 4) Visualize the most common word counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5LB00uyZKV5",
        "outputId": "bdd9fb34-96f5-46d4-ec23-74e37cadca19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "##### Your Code Here #####\n",
        "word_counts.mean().sort_values(ascending=False).head(20), word_counts.sum().sort_values(ascending=False).head(20)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(experience     4.352113\n",
              " business       2.845070\n",
              " work           2.725352\n",
              " team           2.319249\n",
              " science        2.248826\n",
              " learning       2.157277\n",
              " analytics      1.727700\n",
              " machine        1.640845\n",
              " skills         1.636150\n",
              " analysis       1.596244\n",
              " models         1.448357\n",
              " product        1.368545\n",
              " statistical    1.352113\n",
              " solutions      1.244131\n",
              " new            1.241784\n",
              " ability        1.204225\n",
              " scientist      1.199531\n",
              " insights       1.089202\n",
              " help           1.075117\n",
              " technical      1.063380\n",
              " dtype: float64, experience     1854\n",
              " business       1212\n",
              " work           1161\n",
              " team            988\n",
              " science         958\n",
              " learning        919\n",
              " analytics       736\n",
              " machine         699\n",
              " skills          697\n",
              " analysis        680\n",
              " models          617\n",
              " product         583\n",
              " statistical     576\n",
              " solutions       530\n",
              " new             529\n",
              " ability         513\n",
              " scientist       511\n",
              " insights        464\n",
              " help            458\n",
              " technical       453\n",
              " dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwFsTqrVZMYi"
      },
      "source": [
        "## 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WQzk7oITSK-"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBWj1sCYUux8"
      },
      "source": [
        "def tokenize(document):\n",
        "  doc = nlp(document)\n",
        "\n",
        "  return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True)]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gx2gZCbl5Np",
        "outputId": "718eac83-bfef-4150-9e3a-ab32641852b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "##### Your Code Here #####\n",
        "tfidf = TfidfVectorizer(stop_words='english',\n",
        "                        max_features=5000,\n",
        "                        tokenizer=tokenize)\n",
        "\n",
        "dtm = tfidf.fit_transform(df['text'])\n",
        "\n",
        "dtm = pd.DataFrame(data=dtm.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "dtm"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>$</th>\n",
              "      <th>+</th>\n",
              "      <th>+2</th>\n",
              "      <th>+3</th>\n",
              "      <th>-5</th>\n",
              "      <th>-PRON-</th>\n",
              "      <th>.net</th>\n",
              "      <th>/or</th>\n",
              "      <th>0</th>\n",
              "      <th>0305</th>\n",
              "      <th>0356</th>\n",
              "      <th>04/18/19</th>\n",
              "      <th>06366</th>\n",
              "      <th>1</th>\n",
              "      <th>1,000</th>\n",
              "      <th>1,200</th>\n",
              "      <th>1,800</th>\n",
              "      <th>1-</th>\n",
              "      <th>1-800-flowers.com</th>\n",
              "      <th>1-to-1</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>100,000</th>\n",
              "      <th>100,908</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>100k</th>\n",
              "      <th>100x</th>\n",
              "      <th>105,000</th>\n",
              "      <th>1079302</th>\n",
              "      <th>10x</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>125,000</th>\n",
              "      <th>1315</th>\n",
              "      <th>14</th>\n",
              "      <th>140,000</th>\n",
              "      <th>15</th>\n",
              "      <th>15000</th>\n",
              "      <th>...</th>\n",
              "      <th>yearsummary</th>\n",
              "      <th>yearthe</th>\n",
              "      <th>yeartitle</th>\n",
              "      <th>yearworking</th>\n",
              "      <th>yes</th>\n",
              "      <th>yeti</th>\n",
              "      <th>yield</th>\n",
              "      <th>york</th>\n",
              "      <th>you\\'d</th>\n",
              "      <th>you\\'ll</th>\n",
              "      <th>you\\'re</th>\n",
              "      <th>you\\'ve</th>\n",
              "      <th>you\\xe2\\x80\\x99</th>\n",
              "      <th>you\\xe2\\x80\\x99d</th>\n",
              "      <th>you\\xe2\\x80\\x99ll</th>\n",
              "      <th>you\\xe2\\x80\\x99re</th>\n",
              "      <th>you\\xe2\\x80\\x99ve</th>\n",
              "      <th>you\\xe2\\x80\\xa6protect</th>\n",
              "      <th>young</th>\n",
              "      <th>yours\\xe2\\x80\\x94is</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yrs</th>\n",
              "      <th>zenreach</th>\n",
              "      <th>zero</th>\n",
              "      <th>zeus</th>\n",
              "      <th>zf</th>\n",
              "      <th>zf\\xe2\\x80\\x99s</th>\n",
              "      <th>zheng</th>\n",
              "      <th>zillow</th>\n",
              "      <th>zillow\\</th>\n",
              "      <th>zogsports</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zuckerberg</th>\n",
              "      <th>zurich</th>\n",
              "      <th>zurich\\xe2\\x80\\x99s</th>\n",
              "      <th>|</th>\n",
              "      <th>||</th>\n",
              "      <th>~$70</th>\n",
              "      <th>~1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.084152</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028381</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.09150</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.054992</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.063003</td>\n",
              "      <td>0.228349</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.11552</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.147792</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>0.175864</td>\n",
              "      <td>0.031870</td>\n",
              "      <td>0.039541</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03187</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.038497</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02969</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>0.229340</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142939</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>0.057329</td>\n",
              "      <td>0.103891</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.098418</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>0.154081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>0.053043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.054495</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426 rows × 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      $         +   +2  ...    |   ||  ~$70   ~1\n",
              "0    0.000000  0.000000  0.000000  0.0  ...  0.0  0.0   0.0  0.0\n",
              "1    0.084152  0.000000  0.028381  0.0  ...  0.0  0.0   0.0  0.0\n",
              "2    0.054992  0.000000  0.000000  0.0  ...  0.0  0.0   0.0  0.0\n",
              "3    0.063003  0.228349  0.000000  0.0  ...  0.0  0.0   0.0  0.0\n",
              "4    0.000000  0.000000  0.147792  0.0  ...  0.0  0.0   0.0  0.0\n",
              "..        ...       ...       ...  ...  ...  ...  ...   ...  ...\n",
              "421  0.175864  0.031870  0.039541  0.0  ...  0.0  0.0   0.0  0.0\n",
              "422  0.229340  0.000000  0.000000  0.0  ...  0.0  0.0   0.0  0.0\n",
              "423  0.057329  0.103891  0.000000  0.0  ...  0.0  0.0   0.0  0.0\n",
              "424  0.154081  0.000000  0.000000  0.0  ...  0.0  0.0   0.0  0.0\n",
              "425  0.053043  0.000000  0.000000  0.0  ...  0.0  0.0   0.0  0.0\n",
              "\n",
              "[426 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouXRH5gfVoAN",
        "outputId": "ea423c96-07bf-4d64-c92f-e55a8596e37f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "dtm.sum().sort_values(ascending=False).head(20)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datum          41.668042\n",
              "               38.060037\n",
              "experience     28.766793\n",
              "work           24.755178\n",
              "data           22.339079\n",
              "business       21.439352\n",
              "team           20.612735\n",
              "product        17.846855\n",
              "model          16.900758\n",
              "science        16.364777\n",
              "analytic       15.852819\n",
              "machine        14.780630\n",
              "analysis       14.483291\n",
              "learning       14.124525\n",
              "customer       12.951620\n",
              "skill          12.866241\n",
              "scientist      12.462172\n",
              "solution       12.294613\n",
              "develop        12.192922\n",
              "statistical    12.171280\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7iwyfWN4EL7"
      },
      "source": [
        "## 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yRh-N8jv02j"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "Ew0HiD1y4EL7"
      },
      "source": [
        "##### Your Code Here #####\n",
        "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
        "nn.fit(dtm)\n",
        "\n",
        "doc_j = [dtm.iloc[201].values]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4jRt0CmwRCz",
        "outputId": "5da37a6a-081d-4d60-c083-e0785f08ff0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nn.kneighbors(doc_j)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 1.11512547, 1.12137352, 1.14245584, 1.14245584]]),\n",
              " array([[201, 406, 327,  50,  90]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMkqKRgJwb5G",
        "outputId": "b7c0ea39-03c9-4d77-edda-905c9e1a48f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "df['text'].iloc[90], df['text'].iloc[201]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Data Scientist (Multiple levels)-19000C1K   No Visa Sponsorship is available for this position.  Preferred Qualifications  Preferred Qualifications  Within the AI Apps team at Oracle we develop and deploy data science solutions at scale and throughout all of Oracle's existing products and services, and are seeking to grow the team with brilliant and diverse individuals with well-crafted technical ability. This is an exciting and challenging role that will stretch your knowledge and curiosity, offering the opportunity to learn new skills and work within an unusually talented, global community at Oracle.   You will encounter a wide variety of data types, from retail and financial transactions to free text, images and video. AI Apps are required to solve business challenges ranging from recommendation systems and dynamic discounting, management of the flow of goods and services, transportation logistics and movement and storage of materials and inventory, accounting and procurement, project management, manufacturing, staff recruiting, handling and optimizing the HR of an organization   This is a hands-on position where you will be empowered to be creative, ambitious and bold, to solve challenging problems and have the potential to directly impact Oracle\\\\xe2\\\\x80\\\\x99s future. The role requires that you have a solid background in machine learning and know how to invent and modify advanced innovative algorithms, applying them to large data sets. You will be a great teammate who is eager to both teach and learn every day, that is enthusiastic and self-motivated to solve useful problems.   More specifically, you will help us solve business and technical problems with robust and statistically sound use of rigorous scientific methodologies and creative use of algorithms using AI, machine (deep, reinforcement) learning and predictive modelling techniques. You should be comfortable in an environment that combines clear problem specifications with sometimes unpredictable situations, carrying unknowns both of technical and functional kind. Given the global setup, you will also be confident and familiar with tools and styles to work remotely in effective ways together with a team located in multiple geographical locations. You will have the opportunity to actively participate as contributor or leader in a team of peer data scientists, understanding the collaborative and transparent relationships with engineering and product teams and the ways of working of an agile environment. When appropriate, we will support and encourage that you strive to publish the best work in the top journals and conferences.   In all, we are looking for ambitious scientists with an uncommon academic background, an ideal blend of coding, machine learning and statistics, a colleague with whom we can share the enjoyment of being curious, the interest in difficult mathematical and algorithmic problems, and the commitment to be innovative in building predictive models as well as in the way society deals with sensitive data.   We would like you to have An advanced degree in Computer Science, Physics, Engineering, Mathematics or similar, and postgraduate experience in AI, machine learning, analytics and/or predictive modelling. Excellent understanding of the mathematical theory behind common machine learning algorithms for solving classification and regression problems in supervised and unsupervised learning, together with practical experience with the relevant open source libraries. Consistency in developing, innovating, and applying advanced algorithms to address practical problems and in building new analytical products of commercial value. Practical experience in feature engineering, evaluation, selection and automation of such tasks, model interpretation and visualization. Robust knowledge and experience with statistical methods, in particular with the estimation of confidence intervals around parameter values and predicted quantities. Domain expertise in industries such as online retail, digital marketing, financial services, insurance, health care, manufacturing, consumer goods, telecommunications. Proficiency with several years\\\\xe2\\\\x80\\\\x99 experience in more than one of Python, R, Java, C, C++, Scala, and robust Linux shell scripting, as well as in using query languages such as SQL and its adaptations. Experience with horizontally scalable data stores such as Hadoop and other NoSQL technologies such as Map Reduce, Spark, HBase, etc., and associated schemas. It would be fantastic if you also have  A PhD degree in a quantitative Science or technical field. Post-doctoral academic research experience in AI and Machine Learning. Experience in a DevOps role or even better in the nascent DataOps role. Deep knowledge of graphical models, Bayesian networks, Gaussian processes, MCMC, hidden Markov models and social network analysis. Expertise with NLP, text processing and modelling, automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, in particular using deep learning techniques for such tasks, employing popular frameworks, including Keras, TensorFlow, MXNet, Torch, Theano, etc. Expertise in Reinforcement Learning and its deep version. Experience in leading and mentoring other data scientists. You will have several opportunities to complete end-to-end execution of the data science process. This will be carried out in a collaborative environment with product and engineering teams, but ranges from understanding business requirements, data discovery and extraction, model development and evaluation, to production pipeline implementation. To do so you will have access to state of the art computational resources, the opportunity to learn by experimenting with technology and the latest open source libraries, a wealth of data to be understood and modelled, and the most friendly and knowledgeable colleagues in a company that offers extraordinary career opportunities for the best.   Join and build the future with us!  Detailed Description and Job Requirements  Designs, develops and programs methods, processes, and systems to consolidate and analyze unstructured, diverse \\\\xe2\\\\x80\\\\x9cbig data\\\\xe2\\\\x80\\\\x9d sources to generate actionable insights and solutions for client services and product enhancement.  Interacts with product and service teams to identify questions and issues for data analysis and experiments. Develops and codes software programs, algorithms and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources. Identifies meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.  Leading contributor individually and as a team member, providing direction and mentoring to others. Work is non-routine and very complex, involving the application of advanced technical/business skills in area of specialization. 8 years relevant work experience. BS/BA preferred.  Oracle will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of San Francisco's Fair Chance Ordinance.  Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.\",\n",
              " \"InternshipOverview Data Science Internships Portland, OR or Seattle, WA These twelve-week internships are scheduled to begin in May/June 2019 Responsibilities & Requirements Cambia Health Solutions is working to create a seamless and frictionless health care experience for consumers nationwide. This presents a unique challenge and opportunity for innovative and disruptive solutions from our Artificial Intelligence team.  Our Data Scientists design, develop, and implement data-driven solutions using machine learning technologies and advanced statistical analyses. You should be passionate about finding insights in data, comfortable with large and fragmented data sets, and command a variety of analytic tools at your disposal.  Internship opportunities are available on the following teams: Natural Language Processing, Deep Learning, Product Development and Clinical Analytics.  Natural Language Processing in Seattle, WA Our NLP team is looking for a passionate, talented and inventive NLP Data Science Intern to help build industry-leading speech and language solutions. Together with a highly multi-disciplinary team of scientists, engineers, strategic partners and subject domain experts, you will work on building a real product with natural language processing and machine learning at its core.  Essential Function of the NLP Data Scientist Internship: Utilize statistical natural language processing to mine unstructured data and create insights Build and optimize cutting-edge natural language understanding systems such as conversational agents (chatbots) Build core in-house NLP components and analytical tools such as document clustering, topic analysis, text classification, named entity recognition, sentiment analysis, and part-of-speech tagging methods for unstructured and semi-structured data Identify and deploy existing machine learning, natural language processing, and information retrieval techniques and systems for knowledge management and discovery, such as using Electronic Medical Records (EMR) data, progress notes, and discharge summaries to identify admitting diagnosis, reason for consultation, clinical history, etc.Identify ways to analyze consumers\\\\xe2\\\\x80\\\\x99 experiences from various communication channels and improve customer satisfaction Cluster and analyze large amounts of user generated content and process data in large-scale environments in Amazon AWS such as EC2, EMR, MapReduce, and PySpark Integrate the NLP pipeline into the production environment, ensure its scalability, and leverage knowledge gained into other projects, modeling, and work practices Design novel algorithms for problem solving, which may include data cleaning, feature selection, statistical modeling, data clustering and classification, text processing, and other machine learning techniques, to solve complex healthcare problems presented by healthcare organizations Collaborate with different functional teams within Cambia and externally to find solutions to problems in healthcare Key Qualifications and Experience for the NLP Data Scientist Internship: Currently enrolled in an undergraduate or graduate degree program focused on Big Data, Computer Science, Data Analytics, Engineering, Math, Statistics, Science or related degree program (preference will be given to graduate students) Candidates who have completed their degree in the last six months are also encouraged to apply Strong analytic and problem-solving skills, including the ability to apply quantitative analysis techniques to business situations including forecasting, descriptive statistics, statistical inference, and multivariate modeling techniques Experience with a good range of NLP techniques, including text processing, tokenization, POS-tagging, parsing, annotation, regular expressions, language modeling, etc. Ability to develop prototypes by manipulating and analyzing complex, high-volume, high-dimensionality data from various sources Expertise in producing, processing, evaluating, and utilizing unstructured/semi-structured data Proficiency in open-source NLP and machine learning toolkits such Stanford CoreNLP, NLTK, Gensim, Mallet, OpenNLP, LingPipe, cTAKES, scikit-learn, NumPy, LIBSVM, MLlib, Theano, TensorFlow, etc. Solid background in statistical learning and clustering techniques for NLP such as HMM, CRF, SVM, MaxEnt, LDA, LSI, and K-Means Must have ML/NLP algorithm implementation experience as well as the ability to modify standard algorithms, e.g., change objective functions, work out the math, and implement Practical ability to visualize data, communicate about data, and utilize data effectively Proficiency in SQL relational databases and/or NoSQL databases Ability to think creatively and to work well both as part of a team and as an individual contributor Eager to learn new algorithms, new application areas, and new tools Excellent oral and written communication skills to effectively interface and communicate with a broad array of internal and external contacts including leadership Strong programming skills in at least one object oriented programming language, e.g., Java, Python, C++, Scala, etc. Fluency with Linux/Unix Required minimum cumulative undergraduate GPA of 3.0  Deep Learning in Portland, OR The Deep Learning team is looking for brilliant internship candidates in machine learning (ML) and data science with strong software engineering skills, or research background in ML, convex optimization or deep learning. The Deep Learning Data Science Intern will join a team that carries out applied research in ML. It designs, develops and deploys models to serve the rest of Cambia. Our team has developed and applied various ML algorithms and solutions including neural networks, regression models, decision tree-based algorithms, graphical and topic models and time series analyses for various healthcare problems. This team tackles hard issues in healthcare and tries to solve them by the state-of-the-art ML techniques. If the existing methods are not good enough, then we build better ones.  Essential Function of the Deep Learning Data Scientist Internship: Working with large data sets in distributed storages and designing analytical approaches Understanding the concepts and building algorithms in the healthcare domain. Having a broad and deep knowledge of ML methods to make contributions to the future roadmap for the healthcare problem solving platform. Applying coding skills and knowledge data structures and algorithms to develop projects in partnership with other scientists & engineers in the team. Adapt machine learning (e.g. neural network) algorithms to best exploit modern parallel environments (e.g. EC2 with GPU) Key Qualifications and Experience for the Deep Learning Data Scientist Internship: Recent graduate or currently enrolled in graduate degree program focused on machine learning, big data, computer science, statistics, bioinformatics, computational linguistics or other related programs. (Individuals who have completed their degree in the last eighteen months are also encouraged to apply.) Minimum 3.0 cumulative undergraduate grade point average Broad and deep knowledge of machine learning, or deep learning research background. Experience with modern deep learning frameworks (e.g. Tensorflow/Theano/Pytorch/CNTK). Programming proficiency in Python, C/C++, Java, and/or C#. Able to query and manipulate structured and unstructured data using SQL, Hadoop (Spark, HIVE, PIG), and / or scripting languages. Interest and passion for deep learning and distributed representations. Demonstrated ability to analyze large datasets. Practical ability to visualize data, to communicate effectively about data, and to use data effectively Interpersonal skills, cross-group and cross-culture collaboration Problem solving ability and proven track record of achieving results Previous software engineer experience (can be from a previous internship, work experience, coding competitions, or publications). Excellent oral and written communication skills Ability to think creatively, and to work well both as part of a team and as an individual contributor Demonstrated ability to work with minimal direction, with the ability to coordinate complex activities Product Development in Portland, OR The Data Science Product Management Intern will contribute to our team\\\\xe2\\\\x80\\\\x99s focus on comprehensive product strategy from product conception and definition through the product lifecycle. Our small agile team draws from the data science & advanced analytics background of our team members to deliver on all phases of product development from competitive market analysis and ideation, through strategy creation and execution, and ongoing monitoring and communication of implementation tactics and results.  Essential Function of the Product Development Data Scientist Internship: Document initiatives with marketing, UX, engineering, data science & advanced analytics team Understands the target market and primary user needs for a product Assist in the timely completion of product analyses, business case development, user experience design and research, development of formal product plans, presentations, implementation of product changes/new products, and coordinating development and launch activities with the data science, advanced analytics & engineering teams Synthesize business requirements from multiple sources including or may include customer segments, healthcare product experts, brokers, and sales teams into requirement documents and agile project plans Define the feature sets, and business requirements from multiple sources, focusing on the success criteria and the product roadmap (short term & long term) May assist with market research by working with internal teams and visits to customers and other relevant companies with an emphasis on data science & analytic solutions Assist with products life-cycle from planning to tactical activities to cross company go to market planning Collaborates with team members to develop and deploy strategies for improving customer acquisition, engagement, and retention Assist in identifying, reporting, tracking and resolving issues in a timely manner Key Qualifications and Experience for the Deep Learning Data Scientist Internship: Understanding of advanced analytics (e.g., statistics, simulation, optimization) and methods such as time series analysis, longitudinal studies, and life event modeling, as well as data mining methodologies, natural language processing, and machine learning algorithms (e.g. regression, clustering, neural networks, kernel methods, dimensionality reduction, ensemble methods, decision tree methods) Prior knowledge creating successful data-driven products based on a balance of user needs, business goals, and technical constraints Prior experience working with groups of diverse stakeholders; prior experience building consensus a plus Practical ability to visualize data and analytical results, and excellent communication skills to effectively collaborate and communicate with a broad array of internal and external contacts Excellent oral and written communication skills to effectively interface and communicate with a broad array of internal and external contacts including leadership Demonstrated ability to work with minimal direction, with the ability to coordinate complex activities & obtain resources needed to move initiatives forward, in additional to the ability to work within cross-functional teams Knowledge of Agile methodology and product management best practices a plus Demonstrated excellent attention to detail including proven ability to manage multiple projects and priorities simultaneously Ability to learn new technology concepts quickly, think strategically and execute methodically while working in a fast-paced environment where continuous innovation is desired Currently enrolled through spring 2018 in a graduate program focused on Analytics, Computer Science, Data Science, Econometrics, Engineering, Healthcare-related fields, Mathematics, Operations Research, Physics, Product Management, Statistics, or related graduate degree program Minimum 3.0 cumulative undergraduate grade point average Data Science Clinical Analytics in Portland, OR The Data Science Clinical Analytics Intern will contribute to our team\\\\xe2\\\\x80\\\\x99s focus on designing, developing, and implementing data-driven clinical solutions using advanced statistical analyses. Our small agile team draws from the background of our team members to deliver insightful expertise with the goal of delivering data-driven clinical solutions across the organization.  In addition to quantitative and clinical mastery, the ideal candidate will assist our team by effectively working with business partners to identify problems, design novel solutions, interpret and communicate analytical results to varied audiences, and provide analytic support for companywide process improvement efforts.  Essential Function of the Data Science Clinical Analytics Data Scientist Internship: Research, design, develop, and implement data driven solutions using advanced statistical methods Work as a key part of cross-functional teams with various customer groups to study business cases, identify business problems and formulate desired outcomes and solutions Assess existing and identify new sources of data relevant to the problems being investigated, apply statistical data quality procedures to new data sources Generate and test working hypotheses: aggregate and mine data, conduct analyses, and extract actionable results Perform data studies and discovery around new data sources or new uses for existing sources by designing and building large and complex data sets Collect, measure, and interpret process performance data; using tools such as Pareto charts, flow charts, process maps, Cause and Effect diagrams, scatter plots, histograms, and control charts Perform statistical analyses with existing data sets. From results, create visual representations and summary reports of data findings in a variety of formats. Create influential dashboards and presentations that use information to influence senior leadership on business trends and strategies. Prepare and present regular and ad-hoc analysis to internal audiences involved with decisions Key Qualifications and Experience for the Deep Learning Data Scientist Internship: Strong educational background in healthcare fields Experience in working with cross-functional team during full project life cycle Possess the ability to think creatively and demonstrate analytical skills, analyzing complex situations both alone and as part of a team, learning quickly and synthesizing solutions, options and action plans Understanding of advanced analytics (e.g., statistics, simulation, optimization) and methods such as time series analysis, longitudinal and cross-sectional analysis, and life-event modeling Experience with cleaning, aggregating, and pre-processing data from varied sources Demonstrated ability to analyze and interpret qualitative data (research, feedback) and incorporate such insights into quantitative analyses Practical ability to visualize data and analytical results, and excellent communication skills to effectively collaborate and communicate with a broad array of internal and external contacts Experience with at least one statistical/analytical programming tool (R, Python, SAS , etc.) Coursework or practical experience developing analytical models and algorithms Demonstrated ability to apply quantitative analysis techniques to business situations including forecasting, descriptive statistics, statistical inference, and multivariate modeling techniques Strong facilitation skills, including the ability to resolve issues and build consensus among groups of diverse stakeholders Proven ability to provide analysis and data interpretation in support of strategy development, program implementation and evaluation Currently enrolled through spring 2018 in a graduate program focused on Analytics, Computer Science, Data Science, Econometrics, Engineering, Healthcare-related fields, Mathematics, Operations Research, Physics, Product Management, Statistics, or related graduate degree program Minimum 3.0 cumulative undergraduate grade point average  About Us At Cambia, we advocate for transforming the health care system. You aren\\\\xe2\\\\x80\\\\x99t satisfied with the status quo and neither are we. We\\\\'re looking for individuals who are as passionate as we are about transforming the way people experience health care. We offer a competitive salary and a generous benefits package. We are an equal opportunity employer dedicated to workforce diversity and a drug and tobacco-free workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, age, sex, sexual orientation, gender identity, disability, protected veteran status or any other status protected by law. A drug screen and background check is required.  Cambia\\\\xe2\\\\x80\\\\x99s portfolio of companies spans health care information technology and software development; retail health care; health insurance plans that carry the Blue Cross and Blue Shield brands; pharmacy benefit management; life, disability, dental, vision and other lines of protection; alternative solutions to health care access; and free-standing health and wellness solutions.  We have a century of experience in developing and providing health solutions to serve our members. We had our beginnings in the logging communities of the Pacific Northwest as innovators in helping workers afford health care. That pioneering spirit has kept us at the forefront as we build new avenues to improve access to and quality of health care for the future.\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiDfTWceoRkH"
      },
      "source": [
        "## Stretch Goals\n",
        "\n",
        " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
        " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
        " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
        " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
        "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
        " - Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGoMGToodxPy",
        "outputId": "470896d7-5e67-47d7-af7b-f451eda15d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Cosine Similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "dist_matrix = cosine_similarity(dtm)\n",
        "\n",
        "dist_matrix.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(426, 426)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0IG3PdQeiNo",
        "outputId": "acca0436-cf7f-4324-896e-3d46aec5b697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df_dist = pd.DataFrame(dist_matrix)\n",
        "df_dist"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>386</th>\n",
              "      <th>387</th>\n",
              "      <th>388</th>\n",
              "      <th>389</th>\n",
              "      <th>390</th>\n",
              "      <th>391</th>\n",
              "      <th>392</th>\n",
              "      <th>393</th>\n",
              "      <th>394</th>\n",
              "      <th>395</th>\n",
              "      <th>396</th>\n",
              "      <th>397</th>\n",
              "      <th>398</th>\n",
              "      <th>399</th>\n",
              "      <th>400</th>\n",
              "      <th>401</th>\n",
              "      <th>402</th>\n",
              "      <th>403</th>\n",
              "      <th>404</th>\n",
              "      <th>405</th>\n",
              "      <th>406</th>\n",
              "      <th>407</th>\n",
              "      <th>408</th>\n",
              "      <th>409</th>\n",
              "      <th>410</th>\n",
              "      <th>411</th>\n",
              "      <th>412</th>\n",
              "      <th>413</th>\n",
              "      <th>414</th>\n",
              "      <th>415</th>\n",
              "      <th>416</th>\n",
              "      <th>417</th>\n",
              "      <th>418</th>\n",
              "      <th>419</th>\n",
              "      <th>420</th>\n",
              "      <th>421</th>\n",
              "      <th>422</th>\n",
              "      <th>423</th>\n",
              "      <th>424</th>\n",
              "      <th>425</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.055137</td>\n",
              "      <td>0.073173</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>0.062325</td>\n",
              "      <td>0.118530</td>\n",
              "      <td>0.041561</td>\n",
              "      <td>0.041429</td>\n",
              "      <td>0.081422</td>\n",
              "      <td>0.044481</td>\n",
              "      <td>0.056946</td>\n",
              "      <td>0.059618</td>\n",
              "      <td>0.067464</td>\n",
              "      <td>0.041561</td>\n",
              "      <td>0.060051</td>\n",
              "      <td>0.039140</td>\n",
              "      <td>0.038574</td>\n",
              "      <td>0.043631</td>\n",
              "      <td>0.041429</td>\n",
              "      <td>0.050824</td>\n",
              "      <td>0.077124</td>\n",
              "      <td>0.072083</td>\n",
              "      <td>0.083346</td>\n",
              "      <td>0.054718</td>\n",
              "      <td>0.017779</td>\n",
              "      <td>0.062888</td>\n",
              "      <td>0.038699</td>\n",
              "      <td>0.054853</td>\n",
              "      <td>0.067464</td>\n",
              "      <td>0.038702</td>\n",
              "      <td>0.039494</td>\n",
              "      <td>0.089045</td>\n",
              "      <td>0.036627</td>\n",
              "      <td>0.046330</td>\n",
              "      <td>0.092610</td>\n",
              "      <td>0.060743</td>\n",
              "      <td>0.088952</td>\n",
              "      <td>0.051875</td>\n",
              "      <td>0.075841</td>\n",
              "      <td>0.035055</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025924</td>\n",
              "      <td>0.056291</td>\n",
              "      <td>0.046427</td>\n",
              "      <td>0.067463</td>\n",
              "      <td>0.066573</td>\n",
              "      <td>0.065710</td>\n",
              "      <td>0.043444</td>\n",
              "      <td>0.030594</td>\n",
              "      <td>0.037406</td>\n",
              "      <td>0.075198</td>\n",
              "      <td>0.020569</td>\n",
              "      <td>0.046923</td>\n",
              "      <td>0.044458</td>\n",
              "      <td>0.112219</td>\n",
              "      <td>0.095901</td>\n",
              "      <td>0.067472</td>\n",
              "      <td>0.043791</td>\n",
              "      <td>0.110432</td>\n",
              "      <td>0.041161</td>\n",
              "      <td>0.049899</td>\n",
              "      <td>0.078236</td>\n",
              "      <td>0.110089</td>\n",
              "      <td>0.088031</td>\n",
              "      <td>0.038455</td>\n",
              "      <td>0.077787</td>\n",
              "      <td>0.055612</td>\n",
              "      <td>0.055821</td>\n",
              "      <td>0.113079</td>\n",
              "      <td>0.035079</td>\n",
              "      <td>0.050408</td>\n",
              "      <td>0.062062</td>\n",
              "      <td>0.094136</td>\n",
              "      <td>0.052218</td>\n",
              "      <td>0.105672</td>\n",
              "      <td>0.056799</td>\n",
              "      <td>0.034215</td>\n",
              "      <td>0.031254</td>\n",
              "      <td>0.042192</td>\n",
              "      <td>0.083545</td>\n",
              "      <td>0.061724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.055137</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.124194</td>\n",
              "      <td>0.043349</td>\n",
              "      <td>0.041191</td>\n",
              "      <td>0.132916</td>\n",
              "      <td>0.152718</td>\n",
              "      <td>0.085258</td>\n",
              "      <td>0.161986</td>\n",
              "      <td>0.117273</td>\n",
              "      <td>0.090280</td>\n",
              "      <td>0.104268</td>\n",
              "      <td>0.144519</td>\n",
              "      <td>0.152718</td>\n",
              "      <td>0.169964</td>\n",
              "      <td>0.126820</td>\n",
              "      <td>0.108041</td>\n",
              "      <td>0.102123</td>\n",
              "      <td>0.085258</td>\n",
              "      <td>0.093484</td>\n",
              "      <td>0.129274</td>\n",
              "      <td>0.154081</td>\n",
              "      <td>0.145866</td>\n",
              "      <td>0.150964</td>\n",
              "      <td>0.107033</td>\n",
              "      <td>0.113537</td>\n",
              "      <td>0.079999</td>\n",
              "      <td>0.164006</td>\n",
              "      <td>0.144519</td>\n",
              "      <td>0.099823</td>\n",
              "      <td>0.113300</td>\n",
              "      <td>0.183781</td>\n",
              "      <td>0.107209</td>\n",
              "      <td>0.119851</td>\n",
              "      <td>0.128155</td>\n",
              "      <td>0.091524</td>\n",
              "      <td>0.178894</td>\n",
              "      <td>0.136198</td>\n",
              "      <td>0.150950</td>\n",
              "      <td>0.090459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.060569</td>\n",
              "      <td>0.147977</td>\n",
              "      <td>0.200340</td>\n",
              "      <td>0.154597</td>\n",
              "      <td>0.127807</td>\n",
              "      <td>0.150997</td>\n",
              "      <td>0.159185</td>\n",
              "      <td>0.044807</td>\n",
              "      <td>0.170259</td>\n",
              "      <td>0.168067</td>\n",
              "      <td>0.117270</td>\n",
              "      <td>0.125498</td>\n",
              "      <td>0.111200</td>\n",
              "      <td>0.218903</td>\n",
              "      <td>0.141408</td>\n",
              "      <td>0.137199</td>\n",
              "      <td>0.108191</td>\n",
              "      <td>0.206965</td>\n",
              "      <td>0.152877</td>\n",
              "      <td>0.142311</td>\n",
              "      <td>0.182034</td>\n",
              "      <td>0.145667</td>\n",
              "      <td>0.095768</td>\n",
              "      <td>0.130961</td>\n",
              "      <td>0.192452</td>\n",
              "      <td>0.169525</td>\n",
              "      <td>0.115001</td>\n",
              "      <td>0.185651</td>\n",
              "      <td>0.099952</td>\n",
              "      <td>0.082657</td>\n",
              "      <td>0.136923</td>\n",
              "      <td>0.212619</td>\n",
              "      <td>0.081816</td>\n",
              "      <td>0.143740</td>\n",
              "      <td>0.147863</td>\n",
              "      <td>0.132801</td>\n",
              "      <td>0.130872</td>\n",
              "      <td>0.102587</td>\n",
              "      <td>0.146998</td>\n",
              "      <td>0.119354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.073173</td>\n",
              "      <td>0.124194</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.025487</td>\n",
              "      <td>0.107774</td>\n",
              "      <td>0.163623</td>\n",
              "      <td>0.125424</td>\n",
              "      <td>0.111825</td>\n",
              "      <td>0.165801</td>\n",
              "      <td>0.066786</td>\n",
              "      <td>0.082073</td>\n",
              "      <td>0.083617</td>\n",
              "      <td>0.111839</td>\n",
              "      <td>0.125424</td>\n",
              "      <td>0.163538</td>\n",
              "      <td>0.083623</td>\n",
              "      <td>0.055923</td>\n",
              "      <td>0.097020</td>\n",
              "      <td>0.111825</td>\n",
              "      <td>0.118923</td>\n",
              "      <td>0.116606</td>\n",
              "      <td>0.111801</td>\n",
              "      <td>0.138166</td>\n",
              "      <td>0.080603</td>\n",
              "      <td>0.070244</td>\n",
              "      <td>0.123253</td>\n",
              "      <td>0.092434</td>\n",
              "      <td>0.095000</td>\n",
              "      <td>0.111839</td>\n",
              "      <td>0.094069</td>\n",
              "      <td>0.078767</td>\n",
              "      <td>0.190258</td>\n",
              "      <td>0.063536</td>\n",
              "      <td>0.149672</td>\n",
              "      <td>0.156241</td>\n",
              "      <td>0.087001</td>\n",
              "      <td>0.175573</td>\n",
              "      <td>0.123831</td>\n",
              "      <td>0.134130</td>\n",
              "      <td>0.118199</td>\n",
              "      <td>...</td>\n",
              "      <td>0.085937</td>\n",
              "      <td>0.112933</td>\n",
              "      <td>0.172446</td>\n",
              "      <td>0.201624</td>\n",
              "      <td>0.166919</td>\n",
              "      <td>0.148447</td>\n",
              "      <td>0.153394</td>\n",
              "      <td>0.084055</td>\n",
              "      <td>0.125822</td>\n",
              "      <td>0.181621</td>\n",
              "      <td>0.060305</td>\n",
              "      <td>0.130941</td>\n",
              "      <td>0.072258</td>\n",
              "      <td>0.157755</td>\n",
              "      <td>0.138767</td>\n",
              "      <td>0.100880</td>\n",
              "      <td>0.106872</td>\n",
              "      <td>0.174556</td>\n",
              "      <td>0.131306</td>\n",
              "      <td>0.138696</td>\n",
              "      <td>0.167793</td>\n",
              "      <td>0.251727</td>\n",
              "      <td>0.156320</td>\n",
              "      <td>0.138314</td>\n",
              "      <td>0.196910</td>\n",
              "      <td>0.151860</td>\n",
              "      <td>0.112971</td>\n",
              "      <td>0.146163</td>\n",
              "      <td>0.142631</td>\n",
              "      <td>0.135957</td>\n",
              "      <td>0.088835</td>\n",
              "      <td>0.087679</td>\n",
              "      <td>0.087315</td>\n",
              "      <td>0.205511</td>\n",
              "      <td>0.118750</td>\n",
              "      <td>0.092606</td>\n",
              "      <td>0.085078</td>\n",
              "      <td>0.058963</td>\n",
              "      <td>0.143729</td>\n",
              "      <td>0.123585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.002670</td>\n",
              "      <td>0.043349</td>\n",
              "      <td>0.025487</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.014088</td>\n",
              "      <td>0.048396</td>\n",
              "      <td>0.041879</td>\n",
              "      <td>0.074890</td>\n",
              "      <td>0.047746</td>\n",
              "      <td>0.085437</td>\n",
              "      <td>0.032843</td>\n",
              "      <td>0.062594</td>\n",
              "      <td>0.036515</td>\n",
              "      <td>0.041879</td>\n",
              "      <td>0.073655</td>\n",
              "      <td>0.024386</td>\n",
              "      <td>0.110418</td>\n",
              "      <td>0.069510</td>\n",
              "      <td>0.074890</td>\n",
              "      <td>0.105018</td>\n",
              "      <td>0.118893</td>\n",
              "      <td>0.104499</td>\n",
              "      <td>0.091332</td>\n",
              "      <td>0.055449</td>\n",
              "      <td>0.066842</td>\n",
              "      <td>0.047921</td>\n",
              "      <td>0.024413</td>\n",
              "      <td>0.062519</td>\n",
              "      <td>0.036515</td>\n",
              "      <td>0.082248</td>\n",
              "      <td>0.064960</td>\n",
              "      <td>0.121739</td>\n",
              "      <td>0.092682</td>\n",
              "      <td>0.032159</td>\n",
              "      <td>0.052196</td>\n",
              "      <td>0.098691</td>\n",
              "      <td>0.085814</td>\n",
              "      <td>0.047208</td>\n",
              "      <td>0.121472</td>\n",
              "      <td>0.061402</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040520</td>\n",
              "      <td>0.064186</td>\n",
              "      <td>0.112479</td>\n",
              "      <td>0.057387</td>\n",
              "      <td>0.040307</td>\n",
              "      <td>0.077367</td>\n",
              "      <td>0.073120</td>\n",
              "      <td>0.031305</td>\n",
              "      <td>0.070793</td>\n",
              "      <td>0.095365</td>\n",
              "      <td>0.126703</td>\n",
              "      <td>0.100714</td>\n",
              "      <td>0.032021</td>\n",
              "      <td>0.075075</td>\n",
              "      <td>0.065863</td>\n",
              "      <td>0.083318</td>\n",
              "      <td>0.140140</td>\n",
              "      <td>0.072668</td>\n",
              "      <td>0.137053</td>\n",
              "      <td>0.045714</td>\n",
              "      <td>0.130821</td>\n",
              "      <td>0.062247</td>\n",
              "      <td>0.030851</td>\n",
              "      <td>0.053315</td>\n",
              "      <td>0.085860</td>\n",
              "      <td>0.108121</td>\n",
              "      <td>0.073254</td>\n",
              "      <td>0.065557</td>\n",
              "      <td>0.053142</td>\n",
              "      <td>0.039303</td>\n",
              "      <td>0.043734</td>\n",
              "      <td>0.055466</td>\n",
              "      <td>0.017456</td>\n",
              "      <td>0.055379</td>\n",
              "      <td>0.041409</td>\n",
              "      <td>0.075004</td>\n",
              "      <td>0.042932</td>\n",
              "      <td>0.114021</td>\n",
              "      <td>0.092734</td>\n",
              "      <td>0.101144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.062325</td>\n",
              "      <td>0.041191</td>\n",
              "      <td>0.107774</td>\n",
              "      <td>0.014088</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.074691</td>\n",
              "      <td>0.048059</td>\n",
              "      <td>0.033226</td>\n",
              "      <td>0.082440</td>\n",
              "      <td>0.038828</td>\n",
              "      <td>0.034888</td>\n",
              "      <td>0.039652</td>\n",
              "      <td>0.080259</td>\n",
              "      <td>0.048059</td>\n",
              "      <td>0.072412</td>\n",
              "      <td>0.065220</td>\n",
              "      <td>0.033439</td>\n",
              "      <td>0.025377</td>\n",
              "      <td>0.033226</td>\n",
              "      <td>0.101955</td>\n",
              "      <td>0.038912</td>\n",
              "      <td>0.050793</td>\n",
              "      <td>0.069671</td>\n",
              "      <td>0.040331</td>\n",
              "      <td>0.057549</td>\n",
              "      <td>0.063951</td>\n",
              "      <td>0.039505</td>\n",
              "      <td>0.053055</td>\n",
              "      <td>0.080259</td>\n",
              "      <td>0.076029</td>\n",
              "      <td>0.024432</td>\n",
              "      <td>0.082104</td>\n",
              "      <td>0.049556</td>\n",
              "      <td>0.113804</td>\n",
              "      <td>0.102830</td>\n",
              "      <td>0.026901</td>\n",
              "      <td>0.103216</td>\n",
              "      <td>0.039342</td>\n",
              "      <td>0.066982</td>\n",
              "      <td>0.039793</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031841</td>\n",
              "      <td>0.070207</td>\n",
              "      <td>0.058179</td>\n",
              "      <td>0.096563</td>\n",
              "      <td>0.101742</td>\n",
              "      <td>0.052920</td>\n",
              "      <td>0.059885</td>\n",
              "      <td>0.035082</td>\n",
              "      <td>0.012835</td>\n",
              "      <td>0.114280</td>\n",
              "      <td>0.049638</td>\n",
              "      <td>0.072401</td>\n",
              "      <td>0.015300</td>\n",
              "      <td>0.102808</td>\n",
              "      <td>0.103776</td>\n",
              "      <td>0.041518</td>\n",
              "      <td>0.069297</td>\n",
              "      <td>0.093767</td>\n",
              "      <td>0.039857</td>\n",
              "      <td>0.055657</td>\n",
              "      <td>0.060566</td>\n",
              "      <td>0.094594</td>\n",
              "      <td>0.052953</td>\n",
              "      <td>0.112934</td>\n",
              "      <td>0.142937</td>\n",
              "      <td>0.080085</td>\n",
              "      <td>0.059456</td>\n",
              "      <td>0.081449</td>\n",
              "      <td>0.043788</td>\n",
              "      <td>0.056374</td>\n",
              "      <td>0.037100</td>\n",
              "      <td>0.123604</td>\n",
              "      <td>0.043512</td>\n",
              "      <td>0.113035</td>\n",
              "      <td>0.028723</td>\n",
              "      <td>0.049055</td>\n",
              "      <td>0.017619</td>\n",
              "      <td>0.055229</td>\n",
              "      <td>0.051295</td>\n",
              "      <td>0.064628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>0.034215</td>\n",
              "      <td>0.132801</td>\n",
              "      <td>0.092606</td>\n",
              "      <td>0.075004</td>\n",
              "      <td>0.049055</td>\n",
              "      <td>0.109665</td>\n",
              "      <td>0.127444</td>\n",
              "      <td>0.080865</td>\n",
              "      <td>0.134211</td>\n",
              "      <td>0.112743</td>\n",
              "      <td>0.065058</td>\n",
              "      <td>0.134457</td>\n",
              "      <td>0.112503</td>\n",
              "      <td>0.127444</td>\n",
              "      <td>0.159285</td>\n",
              "      <td>0.090390</td>\n",
              "      <td>0.136485</td>\n",
              "      <td>0.077657</td>\n",
              "      <td>0.080865</td>\n",
              "      <td>0.118935</td>\n",
              "      <td>0.102290</td>\n",
              "      <td>0.161530</td>\n",
              "      <td>0.130585</td>\n",
              "      <td>0.134625</td>\n",
              "      <td>0.050145</td>\n",
              "      <td>0.119370</td>\n",
              "      <td>0.123677</td>\n",
              "      <td>0.131301</td>\n",
              "      <td>0.112503</td>\n",
              "      <td>0.077275</td>\n",
              "      <td>0.101883</td>\n",
              "      <td>0.146224</td>\n",
              "      <td>0.105294</td>\n",
              "      <td>0.161151</td>\n",
              "      <td>0.160193</td>\n",
              "      <td>0.135489</td>\n",
              "      <td>0.145559</td>\n",
              "      <td>0.103530</td>\n",
              "      <td>0.138523</td>\n",
              "      <td>0.078992</td>\n",
              "      <td>...</td>\n",
              "      <td>0.057829</td>\n",
              "      <td>0.167735</td>\n",
              "      <td>0.134023</td>\n",
              "      <td>0.169750</td>\n",
              "      <td>0.108801</td>\n",
              "      <td>0.091693</td>\n",
              "      <td>0.131092</td>\n",
              "      <td>0.043668</td>\n",
              "      <td>0.189644</td>\n",
              "      <td>0.172811</td>\n",
              "      <td>0.135484</td>\n",
              "      <td>0.170934</td>\n",
              "      <td>0.086690</td>\n",
              "      <td>0.165425</td>\n",
              "      <td>0.124650</td>\n",
              "      <td>0.153741</td>\n",
              "      <td>0.155027</td>\n",
              "      <td>0.158729</td>\n",
              "      <td>0.157540</td>\n",
              "      <td>0.173254</td>\n",
              "      <td>0.161455</td>\n",
              "      <td>0.116032</td>\n",
              "      <td>0.105192</td>\n",
              "      <td>0.110442</td>\n",
              "      <td>0.179536</td>\n",
              "      <td>0.163110</td>\n",
              "      <td>0.135594</td>\n",
              "      <td>0.180094</td>\n",
              "      <td>0.163721</td>\n",
              "      <td>0.098398</td>\n",
              "      <td>0.130325</td>\n",
              "      <td>0.152950</td>\n",
              "      <td>0.085485</td>\n",
              "      <td>0.137404</td>\n",
              "      <td>0.130073</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.124599</td>\n",
              "      <td>0.107322</td>\n",
              "      <td>0.165613</td>\n",
              "      <td>0.106177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>0.031254</td>\n",
              "      <td>0.130872</td>\n",
              "      <td>0.085078</td>\n",
              "      <td>0.042932</td>\n",
              "      <td>0.017619</td>\n",
              "      <td>0.075015</td>\n",
              "      <td>0.100469</td>\n",
              "      <td>0.072849</td>\n",
              "      <td>0.216573</td>\n",
              "      <td>0.102181</td>\n",
              "      <td>0.068592</td>\n",
              "      <td>0.072588</td>\n",
              "      <td>0.092387</td>\n",
              "      <td>0.100469</td>\n",
              "      <td>0.183049</td>\n",
              "      <td>0.085063</td>\n",
              "      <td>0.117857</td>\n",
              "      <td>0.085159</td>\n",
              "      <td>0.072849</td>\n",
              "      <td>0.073286</td>\n",
              "      <td>0.078778</td>\n",
              "      <td>0.146581</td>\n",
              "      <td>0.107424</td>\n",
              "      <td>0.124157</td>\n",
              "      <td>0.066147</td>\n",
              "      <td>0.156779</td>\n",
              "      <td>0.080553</td>\n",
              "      <td>0.113702</td>\n",
              "      <td>0.092387</td>\n",
              "      <td>0.075317</td>\n",
              "      <td>0.121025</td>\n",
              "      <td>0.132763</td>\n",
              "      <td>0.061383</td>\n",
              "      <td>0.093334</td>\n",
              "      <td>0.134486</td>\n",
              "      <td>0.153930</td>\n",
              "      <td>0.150355</td>\n",
              "      <td>0.156585</td>\n",
              "      <td>0.094207</td>\n",
              "      <td>0.085389</td>\n",
              "      <td>...</td>\n",
              "      <td>0.057127</td>\n",
              "      <td>0.136943</td>\n",
              "      <td>0.134696</td>\n",
              "      <td>0.128223</td>\n",
              "      <td>0.086750</td>\n",
              "      <td>0.095471</td>\n",
              "      <td>0.131412</td>\n",
              "      <td>0.021392</td>\n",
              "      <td>0.158993</td>\n",
              "      <td>0.145935</td>\n",
              "      <td>0.121482</td>\n",
              "      <td>0.095959</td>\n",
              "      <td>0.090039</td>\n",
              "      <td>0.123860</td>\n",
              "      <td>0.106241</td>\n",
              "      <td>0.120285</td>\n",
              "      <td>0.107857</td>\n",
              "      <td>0.153636</td>\n",
              "      <td>0.174360</td>\n",
              "      <td>0.149597</td>\n",
              "      <td>0.095556</td>\n",
              "      <td>0.093162</td>\n",
              "      <td>0.081398</td>\n",
              "      <td>0.066958</td>\n",
              "      <td>0.134878</td>\n",
              "      <td>0.138179</td>\n",
              "      <td>0.125659</td>\n",
              "      <td>0.153915</td>\n",
              "      <td>0.100174</td>\n",
              "      <td>0.113718</td>\n",
              "      <td>0.103438</td>\n",
              "      <td>0.104678</td>\n",
              "      <td>0.076530</td>\n",
              "      <td>0.225781</td>\n",
              "      <td>0.285274</td>\n",
              "      <td>0.124599</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.079450</td>\n",
              "      <td>0.130401</td>\n",
              "      <td>0.081909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>0.042192</td>\n",
              "      <td>0.102587</td>\n",
              "      <td>0.058963</td>\n",
              "      <td>0.114021</td>\n",
              "      <td>0.055229</td>\n",
              "      <td>0.064880</td>\n",
              "      <td>0.087428</td>\n",
              "      <td>0.067431</td>\n",
              "      <td>0.108809</td>\n",
              "      <td>0.108989</td>\n",
              "      <td>0.051549</td>\n",
              "      <td>0.074388</td>\n",
              "      <td>0.091836</td>\n",
              "      <td>0.087428</td>\n",
              "      <td>0.088784</td>\n",
              "      <td>0.102213</td>\n",
              "      <td>0.152091</td>\n",
              "      <td>0.051384</td>\n",
              "      <td>0.067431</td>\n",
              "      <td>0.109485</td>\n",
              "      <td>0.142239</td>\n",
              "      <td>0.107916</td>\n",
              "      <td>0.073101</td>\n",
              "      <td>0.108412</td>\n",
              "      <td>0.035441</td>\n",
              "      <td>0.073180</td>\n",
              "      <td>0.069421</td>\n",
              "      <td>0.088648</td>\n",
              "      <td>0.091836</td>\n",
              "      <td>0.068057</td>\n",
              "      <td>0.084309</td>\n",
              "      <td>0.148448</td>\n",
              "      <td>0.111881</td>\n",
              "      <td>0.079874</td>\n",
              "      <td>0.124905</td>\n",
              "      <td>0.091101</td>\n",
              "      <td>0.101883</td>\n",
              "      <td>0.066241</td>\n",
              "      <td>0.113540</td>\n",
              "      <td>0.102983</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061585</td>\n",
              "      <td>0.130615</td>\n",
              "      <td>0.117906</td>\n",
              "      <td>0.092327</td>\n",
              "      <td>0.066585</td>\n",
              "      <td>0.089480</td>\n",
              "      <td>0.111152</td>\n",
              "      <td>0.040501</td>\n",
              "      <td>0.096440</td>\n",
              "      <td>0.141164</td>\n",
              "      <td>0.101519</td>\n",
              "      <td>0.171575</td>\n",
              "      <td>0.054031</td>\n",
              "      <td>0.117074</td>\n",
              "      <td>0.107057</td>\n",
              "      <td>0.105305</td>\n",
              "      <td>0.114755</td>\n",
              "      <td>0.106232</td>\n",
              "      <td>0.095444</td>\n",
              "      <td>0.075412</td>\n",
              "      <td>0.146530</td>\n",
              "      <td>0.067357</td>\n",
              "      <td>0.067811</td>\n",
              "      <td>0.074090</td>\n",
              "      <td>0.134427</td>\n",
              "      <td>0.166272</td>\n",
              "      <td>0.088448</td>\n",
              "      <td>0.122242</td>\n",
              "      <td>0.055164</td>\n",
              "      <td>0.056599</td>\n",
              "      <td>0.077178</td>\n",
              "      <td>0.094894</td>\n",
              "      <td>0.042866</td>\n",
              "      <td>0.075995</td>\n",
              "      <td>0.106973</td>\n",
              "      <td>0.107322</td>\n",
              "      <td>0.079450</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.118334</td>\n",
              "      <td>0.109295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>0.083545</td>\n",
              "      <td>0.146998</td>\n",
              "      <td>0.143729</td>\n",
              "      <td>0.092734</td>\n",
              "      <td>0.051295</td>\n",
              "      <td>0.089256</td>\n",
              "      <td>0.127095</td>\n",
              "      <td>0.116849</td>\n",
              "      <td>0.167373</td>\n",
              "      <td>0.108376</td>\n",
              "      <td>0.119363</td>\n",
              "      <td>0.095456</td>\n",
              "      <td>0.105869</td>\n",
              "      <td>0.127095</td>\n",
              "      <td>0.177346</td>\n",
              "      <td>0.101487</td>\n",
              "      <td>0.179212</td>\n",
              "      <td>0.083618</td>\n",
              "      <td>0.116849</td>\n",
              "      <td>0.123555</td>\n",
              "      <td>0.171062</td>\n",
              "      <td>0.159339</td>\n",
              "      <td>0.150823</td>\n",
              "      <td>0.131214</td>\n",
              "      <td>0.042241</td>\n",
              "      <td>0.122693</td>\n",
              "      <td>0.157757</td>\n",
              "      <td>0.125952</td>\n",
              "      <td>0.105869</td>\n",
              "      <td>0.090005</td>\n",
              "      <td>0.098147</td>\n",
              "      <td>0.183113</td>\n",
              "      <td>0.126312</td>\n",
              "      <td>0.145389</td>\n",
              "      <td>0.169399</td>\n",
              "      <td>0.171784</td>\n",
              "      <td>0.166733</td>\n",
              "      <td>0.133544</td>\n",
              "      <td>0.189106</td>\n",
              "      <td>0.111830</td>\n",
              "      <td>...</td>\n",
              "      <td>0.102523</td>\n",
              "      <td>0.149384</td>\n",
              "      <td>0.157121</td>\n",
              "      <td>0.196188</td>\n",
              "      <td>0.121504</td>\n",
              "      <td>0.120570</td>\n",
              "      <td>0.195460</td>\n",
              "      <td>0.046776</td>\n",
              "      <td>0.158088</td>\n",
              "      <td>0.216775</td>\n",
              "      <td>0.143420</td>\n",
              "      <td>0.151213</td>\n",
              "      <td>0.131213</td>\n",
              "      <td>0.231809</td>\n",
              "      <td>0.147308</td>\n",
              "      <td>0.158929</td>\n",
              "      <td>0.158818</td>\n",
              "      <td>0.199490</td>\n",
              "      <td>0.158537</td>\n",
              "      <td>0.136096</td>\n",
              "      <td>0.196122</td>\n",
              "      <td>0.119895</td>\n",
              "      <td>0.131231</td>\n",
              "      <td>0.094604</td>\n",
              "      <td>0.218935</td>\n",
              "      <td>0.171962</td>\n",
              "      <td>0.140818</td>\n",
              "      <td>0.168667</td>\n",
              "      <td>0.126540</td>\n",
              "      <td>0.122766</td>\n",
              "      <td>0.136571</td>\n",
              "      <td>0.145191</td>\n",
              "      <td>0.075020</td>\n",
              "      <td>0.168449</td>\n",
              "      <td>0.173560</td>\n",
              "      <td>0.165613</td>\n",
              "      <td>0.130401</td>\n",
              "      <td>0.118334</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.170194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>0.061724</td>\n",
              "      <td>0.119354</td>\n",
              "      <td>0.123585</td>\n",
              "      <td>0.101144</td>\n",
              "      <td>0.064628</td>\n",
              "      <td>0.107458</td>\n",
              "      <td>0.111236</td>\n",
              "      <td>0.116281</td>\n",
              "      <td>0.119053</td>\n",
              "      <td>0.109698</td>\n",
              "      <td>0.097457</td>\n",
              "      <td>0.095847</td>\n",
              "      <td>0.161117</td>\n",
              "      <td>0.111236</td>\n",
              "      <td>0.112496</td>\n",
              "      <td>0.081885</td>\n",
              "      <td>0.112169</td>\n",
              "      <td>0.084979</td>\n",
              "      <td>0.116281</td>\n",
              "      <td>0.092636</td>\n",
              "      <td>0.164134</td>\n",
              "      <td>0.135033</td>\n",
              "      <td>0.108863</td>\n",
              "      <td>0.106139</td>\n",
              "      <td>0.093762</td>\n",
              "      <td>0.088740</td>\n",
              "      <td>0.096306</td>\n",
              "      <td>0.130990</td>\n",
              "      <td>0.161117</td>\n",
              "      <td>0.167545</td>\n",
              "      <td>0.104356</td>\n",
              "      <td>0.222171</td>\n",
              "      <td>0.147076</td>\n",
              "      <td>0.113261</td>\n",
              "      <td>0.139545</td>\n",
              "      <td>0.117729</td>\n",
              "      <td>0.182863</td>\n",
              "      <td>0.101840</td>\n",
              "      <td>0.167323</td>\n",
              "      <td>0.085382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073856</td>\n",
              "      <td>0.151034</td>\n",
              "      <td>0.226381</td>\n",
              "      <td>0.166697</td>\n",
              "      <td>0.100937</td>\n",
              "      <td>0.135854</td>\n",
              "      <td>0.114697</td>\n",
              "      <td>0.052882</td>\n",
              "      <td>0.129241</td>\n",
              "      <td>0.143697</td>\n",
              "      <td>0.121276</td>\n",
              "      <td>0.111885</td>\n",
              "      <td>0.083398</td>\n",
              "      <td>0.190887</td>\n",
              "      <td>0.136898</td>\n",
              "      <td>0.088724</td>\n",
              "      <td>0.143572</td>\n",
              "      <td>0.181693</td>\n",
              "      <td>0.170995</td>\n",
              "      <td>0.121635</td>\n",
              "      <td>0.221457</td>\n",
              "      <td>0.114780</td>\n",
              "      <td>0.141792</td>\n",
              "      <td>0.111989</td>\n",
              "      <td>0.172132</td>\n",
              "      <td>0.188978</td>\n",
              "      <td>0.105160</td>\n",
              "      <td>0.184893</td>\n",
              "      <td>0.091790</td>\n",
              "      <td>0.101684</td>\n",
              "      <td>0.080010</td>\n",
              "      <td>0.139398</td>\n",
              "      <td>0.090094</td>\n",
              "      <td>0.115277</td>\n",
              "      <td>0.089721</td>\n",
              "      <td>0.106177</td>\n",
              "      <td>0.081909</td>\n",
              "      <td>0.109295</td>\n",
              "      <td>0.170194</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426 rows × 426 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2    ...       423       424       425\n",
              "0    1.000000  0.055137  0.073173  ...  0.042192  0.083545  0.061724\n",
              "1    0.055137  1.000000  0.124194  ...  0.102587  0.146998  0.119354\n",
              "2    0.073173  0.124194  1.000000  ...  0.058963  0.143729  0.123585\n",
              "3    0.002670  0.043349  0.025487  ...  0.114021  0.092734  0.101144\n",
              "4    0.062325  0.041191  0.107774  ...  0.055229  0.051295  0.064628\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "421  0.034215  0.132801  0.092606  ...  0.107322  0.165613  0.106177\n",
              "422  0.031254  0.130872  0.085078  ...  0.079450  0.130401  0.081909\n",
              "423  0.042192  0.102587  0.058963  ...  1.000000  0.118334  0.109295\n",
              "424  0.083545  0.146998  0.143729  ...  0.118334  1.000000  0.170194\n",
              "425  0.061724  0.119354  0.123585  ...  0.109295  0.170194  1.000000\n",
              "\n",
              "[426 rows x 426 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9NXN6_0e08z",
        "outputId": "ed89a9d4-da0d-4cdc-bd47-70307d38a565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df_dist.mean().sort_values(ascending=False)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201    0.221539\n",
              "327    0.187673\n",
              "331    0.187656\n",
              "50     0.187504\n",
              "90     0.187504\n",
              "         ...   \n",
              "393    0.055832\n",
              "101    0.048008\n",
              "159    0.041851\n",
              "105    0.038441\n",
              "124    0.030746\n",
              "Length: 426, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-yveUmfOJh",
        "outputId": "48a5a022-b903-45ab-ae1b-5e0d31afba8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "df['text'].iloc[201]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"InternshipOverview Data Science Internships Portland, OR or Seattle, WA These twelve-week internships are scheduled to begin in May/June 2019 Responsibilities & Requirements Cambia Health Solutions is working to create a seamless and frictionless health care experience for consumers nationwide. This presents a unique challenge and opportunity for innovative and disruptive solutions from our Artificial Intelligence team.  Our Data Scientists design, develop, and implement data-driven solutions using machine learning technologies and advanced statistical analyses. You should be passionate about finding insights in data, comfortable with large and fragmented data sets, and command a variety of analytic tools at your disposal.  Internship opportunities are available on the following teams: Natural Language Processing, Deep Learning, Product Development and Clinical Analytics.  Natural Language Processing in Seattle, WA Our NLP team is looking for a passionate, talented and inventive NLP Data Science Intern to help build industry-leading speech and language solutions. Together with a highly multi-disciplinary team of scientists, engineers, strategic partners and subject domain experts, you will work on building a real product with natural language processing and machine learning at its core.  Essential Function of the NLP Data Scientist Internship: Utilize statistical natural language processing to mine unstructured data and create insights Build and optimize cutting-edge natural language understanding systems such as conversational agents (chatbots) Build core in-house NLP components and analytical tools such as document clustering, topic analysis, text classification, named entity recognition, sentiment analysis, and part-of-speech tagging methods for unstructured and semi-structured data Identify and deploy existing machine learning, natural language processing, and information retrieval techniques and systems for knowledge management and discovery, such as using Electronic Medical Records (EMR) data, progress notes, and discharge summaries to identify admitting diagnosis, reason for consultation, clinical history, etc.Identify ways to analyze consumers\\\\xe2\\\\x80\\\\x99 experiences from various communication channels and improve customer satisfaction Cluster and analyze large amounts of user generated content and process data in large-scale environments in Amazon AWS such as EC2, EMR, MapReduce, and PySpark Integrate the NLP pipeline into the production environment, ensure its scalability, and leverage knowledge gained into other projects, modeling, and work practices Design novel algorithms for problem solving, which may include data cleaning, feature selection, statistical modeling, data clustering and classification, text processing, and other machine learning techniques, to solve complex healthcare problems presented by healthcare organizations Collaborate with different functional teams within Cambia and externally to find solutions to problems in healthcare Key Qualifications and Experience for the NLP Data Scientist Internship: Currently enrolled in an undergraduate or graduate degree program focused on Big Data, Computer Science, Data Analytics, Engineering, Math, Statistics, Science or related degree program (preference will be given to graduate students) Candidates who have completed their degree in the last six months are also encouraged to apply Strong analytic and problem-solving skills, including the ability to apply quantitative analysis techniques to business situations including forecasting, descriptive statistics, statistical inference, and multivariate modeling techniques Experience with a good range of NLP techniques, including text processing, tokenization, POS-tagging, parsing, annotation, regular expressions, language modeling, etc. Ability to develop prototypes by manipulating and analyzing complex, high-volume, high-dimensionality data from various sources Expertise in producing, processing, evaluating, and utilizing unstructured/semi-structured data Proficiency in open-source NLP and machine learning toolkits such Stanford CoreNLP, NLTK, Gensim, Mallet, OpenNLP, LingPipe, cTAKES, scikit-learn, NumPy, LIBSVM, MLlib, Theano, TensorFlow, etc. Solid background in statistical learning and clustering techniques for NLP such as HMM, CRF, SVM, MaxEnt, LDA, LSI, and K-Means Must have ML/NLP algorithm implementation experience as well as the ability to modify standard algorithms, e.g., change objective functions, work out the math, and implement Practical ability to visualize data, communicate about data, and utilize data effectively Proficiency in SQL relational databases and/or NoSQL databases Ability to think creatively and to work well both as part of a team and as an individual contributor Eager to learn new algorithms, new application areas, and new tools Excellent oral and written communication skills to effectively interface and communicate with a broad array of internal and external contacts including leadership Strong programming skills in at least one object oriented programming language, e.g., Java, Python, C++, Scala, etc. Fluency with Linux/Unix Required minimum cumulative undergraduate GPA of 3.0  Deep Learning in Portland, OR The Deep Learning team is looking for brilliant internship candidates in machine learning (ML) and data science with strong software engineering skills, or research background in ML, convex optimization or deep learning. The Deep Learning Data Science Intern will join a team that carries out applied research in ML. It designs, develops and deploys models to serve the rest of Cambia. Our team has developed and applied various ML algorithms and solutions including neural networks, regression models, decision tree-based algorithms, graphical and topic models and time series analyses for various healthcare problems. This team tackles hard issues in healthcare and tries to solve them by the state-of-the-art ML techniques. If the existing methods are not good enough, then we build better ones.  Essential Function of the Deep Learning Data Scientist Internship: Working with large data sets in distributed storages and designing analytical approaches Understanding the concepts and building algorithms in the healthcare domain. Having a broad and deep knowledge of ML methods to make contributions to the future roadmap for the healthcare problem solving platform. Applying coding skills and knowledge data structures and algorithms to develop projects in partnership with other scientists & engineers in the team. Adapt machine learning (e.g. neural network) algorithms to best exploit modern parallel environments (e.g. EC2 with GPU) Key Qualifications and Experience for the Deep Learning Data Scientist Internship: Recent graduate or currently enrolled in graduate degree program focused on machine learning, big data, computer science, statistics, bioinformatics, computational linguistics or other related programs. (Individuals who have completed their degree in the last eighteen months are also encouraged to apply.) Minimum 3.0 cumulative undergraduate grade point average Broad and deep knowledge of machine learning, or deep learning research background. Experience with modern deep learning frameworks (e.g. Tensorflow/Theano/Pytorch/CNTK). Programming proficiency in Python, C/C++, Java, and/or C#. Able to query and manipulate structured and unstructured data using SQL, Hadoop (Spark, HIVE, PIG), and / or scripting languages. Interest and passion for deep learning and distributed representations. Demonstrated ability to analyze large datasets. Practical ability to visualize data, to communicate effectively about data, and to use data effectively Interpersonal skills, cross-group and cross-culture collaboration Problem solving ability and proven track record of achieving results Previous software engineer experience (can be from a previous internship, work experience, coding competitions, or publications). Excellent oral and written communication skills Ability to think creatively, and to work well both as part of a team and as an individual contributor Demonstrated ability to work with minimal direction, with the ability to coordinate complex activities Product Development in Portland, OR The Data Science Product Management Intern will contribute to our team\\\\xe2\\\\x80\\\\x99s focus on comprehensive product strategy from product conception and definition through the product lifecycle. Our small agile team draws from the data science & advanced analytics background of our team members to deliver on all phases of product development from competitive market analysis and ideation, through strategy creation and execution, and ongoing monitoring and communication of implementation tactics and results.  Essential Function of the Product Development Data Scientist Internship: Document initiatives with marketing, UX, engineering, data science & advanced analytics team Understands the target market and primary user needs for a product Assist in the timely completion of product analyses, business case development, user experience design and research, development of formal product plans, presentations, implementation of product changes/new products, and coordinating development and launch activities with the data science, advanced analytics & engineering teams Synthesize business requirements from multiple sources including or may include customer segments, healthcare product experts, brokers, and sales teams into requirement documents and agile project plans Define the feature sets, and business requirements from multiple sources, focusing on the success criteria and the product roadmap (short term & long term) May assist with market research by working with internal teams and visits to customers and other relevant companies with an emphasis on data science & analytic solutions Assist with products life-cycle from planning to tactical activities to cross company go to market planning Collaborates with team members to develop and deploy strategies for improving customer acquisition, engagement, and retention Assist in identifying, reporting, tracking and resolving issues in a timely manner Key Qualifications and Experience for the Deep Learning Data Scientist Internship: Understanding of advanced analytics (e.g., statistics, simulation, optimization) and methods such as time series analysis, longitudinal studies, and life event modeling, as well as data mining methodologies, natural language processing, and machine learning algorithms (e.g. regression, clustering, neural networks, kernel methods, dimensionality reduction, ensemble methods, decision tree methods) Prior knowledge creating successful data-driven products based on a balance of user needs, business goals, and technical constraints Prior experience working with groups of diverse stakeholders; prior experience building consensus a plus Practical ability to visualize data and analytical results, and excellent communication skills to effectively collaborate and communicate with a broad array of internal and external contacts Excellent oral and written communication skills to effectively interface and communicate with a broad array of internal and external contacts including leadership Demonstrated ability to work with minimal direction, with the ability to coordinate complex activities & obtain resources needed to move initiatives forward, in additional to the ability to work within cross-functional teams Knowledge of Agile methodology and product management best practices a plus Demonstrated excellent attention to detail including proven ability to manage multiple projects and priorities simultaneously Ability to learn new technology concepts quickly, think strategically and execute methodically while working in a fast-paced environment where continuous innovation is desired Currently enrolled through spring 2018 in a graduate program focused on Analytics, Computer Science, Data Science, Econometrics, Engineering, Healthcare-related fields, Mathematics, Operations Research, Physics, Product Management, Statistics, or related graduate degree program Minimum 3.0 cumulative undergraduate grade point average Data Science Clinical Analytics in Portland, OR The Data Science Clinical Analytics Intern will contribute to our team\\\\xe2\\\\x80\\\\x99s focus on designing, developing, and implementing data-driven clinical solutions using advanced statistical analyses. Our small agile team draws from the background of our team members to deliver insightful expertise with the goal of delivering data-driven clinical solutions across the organization.  In addition to quantitative and clinical mastery, the ideal candidate will assist our team by effectively working with business partners to identify problems, design novel solutions, interpret and communicate analytical results to varied audiences, and provide analytic support for companywide process improvement efforts.  Essential Function of the Data Science Clinical Analytics Data Scientist Internship: Research, design, develop, and implement data driven solutions using advanced statistical methods Work as a key part of cross-functional teams with various customer groups to study business cases, identify business problems and formulate desired outcomes and solutions Assess existing and identify new sources of data relevant to the problems being investigated, apply statistical data quality procedures to new data sources Generate and test working hypotheses: aggregate and mine data, conduct analyses, and extract actionable results Perform data studies and discovery around new data sources or new uses for existing sources by designing and building large and complex data sets Collect, measure, and interpret process performance data; using tools such as Pareto charts, flow charts, process maps, Cause and Effect diagrams, scatter plots, histograms, and control charts Perform statistical analyses with existing data sets. From results, create visual representations and summary reports of data findings in a variety of formats. Create influential dashboards and presentations that use information to influence senior leadership on business trends and strategies. Prepare and present regular and ad-hoc analysis to internal audiences involved with decisions Key Qualifications and Experience for the Deep Learning Data Scientist Internship: Strong educational background in healthcare fields Experience in working with cross-functional team during full project life cycle Possess the ability to think creatively and demonstrate analytical skills, analyzing complex situations both alone and as part of a team, learning quickly and synthesizing solutions, options and action plans Understanding of advanced analytics (e.g., statistics, simulation, optimization) and methods such as time series analysis, longitudinal and cross-sectional analysis, and life-event modeling Experience with cleaning, aggregating, and pre-processing data from varied sources Demonstrated ability to analyze and interpret qualitative data (research, feedback) and incorporate such insights into quantitative analyses Practical ability to visualize data and analytical results, and excellent communication skills to effectively collaborate and communicate with a broad array of internal and external contacts Experience with at least one statistical/analytical programming tool (R, Python, SAS , etc.) Coursework or practical experience developing analytical models and algorithms Demonstrated ability to apply quantitative analysis techniques to business situations including forecasting, descriptive statistics, statistical inference, and multivariate modeling techniques Strong facilitation skills, including the ability to resolve issues and build consensus among groups of diverse stakeholders Proven ability to provide analysis and data interpretation in support of strategy development, program implementation and evaluation Currently enrolled through spring 2018 in a graduate program focused on Analytics, Computer Science, Data Science, Econometrics, Engineering, Healthcare-related fields, Mathematics, Operations Research, Physics, Product Management, Statistics, or related graduate degree program Minimum 3.0 cumulative undergraduate grade point average  About Us At Cambia, we advocate for transforming the health care system. You aren\\\\xe2\\\\x80\\\\x99t satisfied with the status quo and neither are we. We\\\\'re looking for individuals who are as passionate as we are about transforming the way people experience health care. We offer a competitive salary and a generous benefits package. We are an equal opportunity employer dedicated to workforce diversity and a drug and tobacco-free workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, age, sex, sexual orientation, gender identity, disability, protected veteran status or any other status protected by law. A drug screen and background check is required.  Cambia\\\\xe2\\\\x80\\\\x99s portfolio of companies spans health care information technology and software development; retail health care; health insurance plans that carry the Blue Cross and Blue Shield brands; pharmacy benefit management; life, disability, dental, vision and other lines of protection; alternative solutions to health care access; and free-standing health and wellness solutions.  We have a century of experience in developing and providing health solutions to serve our members. We had our beginnings in the logging communities of the Pacific Northwest as innovators in helping workers afford health care. That pioneering spirit has kept us at the forefront as we build new avenues to improve access to and quality of health care for the future.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAYjV5Elww3k",
        "outputId": "97ec6e8c-ecdb-4183-c18c-e1326a0384ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "df_dist[201].iloc[90], df_dist[201].iloc[10], df_dist[201].iloc[50], df_dist[201].iloc[406],"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.34739733239064385,\n",
              " 0.1664057237394297,\n",
              " 0.34739733239064385,\n",
              " 0.37824759492399174)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTGrrmZAxM8j",
        "outputId": "6057368b-aea4-4968-8160-6a43ae896a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_dist[201].sort_values(ascending=False).head(10)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201    1.000000\n",
              "406    0.378248\n",
              "327    0.371261\n",
              "90     0.347397\n",
              "50     0.347397\n",
              "388    0.342811\n",
              "407    0.339459\n",
              "410    0.329661\n",
              "331    0.329630\n",
              "153    0.328872\n",
              "Name: 201, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    }
  ]
}